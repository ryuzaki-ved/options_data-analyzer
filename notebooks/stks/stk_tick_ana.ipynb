{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c45861f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "File path: D:\\Market Projects\\options_data_analyzer\\Aug '25\\Aug 07 Exp\\07 Aug\\BEL_EQ.csv\n",
      "Shape: (18250, 5)\n",
      "Columns: ['date', 'price', 'qty', 'trnvr', 'cum_trnvr']\n",
      "\n",
      "First few rows:\n",
      "                     date   price    qty        trnvr    cum_trnvr\n",
      "0  2025-08-07 09:15:00 AM  386.85  65740  25431519.00  25431519.00\n",
      "1  2025-08-07 09:15:01 AM  386.65      0         0.00  25431519.00\n",
      "2  2025-08-07 09:15:01 AM  386.30      0         0.00  25431519.00\n",
      "3  2025-08-07 09:15:01 AM  386.30    895    345738.50  25777257.50\n",
      "4  2025-08-07 09:15:01 AM  386.75   1401    541836.75  26319094.25\n",
      "\n",
      "Data types:\n",
      "date          object\n",
      "price        float64\n",
      "qty            int64\n",
      "trnvr        float64\n",
      "cum_trnvr    float64\n",
      "dtype: object\n",
      "\n",
      "Basic statistics:\n",
      "              price            qty         trnvr     cum_trnvr\n",
      "count  18250.000000   18250.000000  1.825000e+04  1.825000e+04\n",
      "mean     386.537181     630.694301  2.439435e+05  2.317732e+09\n",
      "std        1.570300    2651.792039  1.026096e+06  1.129666e+09\n",
      "min      383.500000       0.000000  0.000000e+00  2.543152e+07\n",
      "25%      385.150000       0.000000  0.000000e+00  1.473757e+09\n",
      "50%      386.400000      10.000000  3.850250e+03  2.248665e+09\n",
      "75%      387.950000     331.000000  1.284426e+05  3.123428e+09\n",
      "max      390.200000  153858.000000  5.955074e+07  4.451969e+09\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the file path\n",
    "file_path = r\"D:\\Market Projects\\options_data_analyzer\\Aug '25\\Aug 07 Exp\\07 Aug\\BEL_EQ.csv\"\n",
    "\n",
    "# Load CSV with pandas\n",
    "# Using default encoding (utf-8) and comma delimiter\n",
    "# The file appears to have standard CSV format\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display basic information about the loaded data\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"File path: {file_path}\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdc1406d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FIRST 10 ROWS ===\n",
      "                     date   price    qty        trnvr    cum_trnvr\n",
      "0  2025-08-07 09:15:00 AM  386.85  65740  25431519.00  25431519.00\n",
      "1  2025-08-07 09:15:01 AM  386.65      0         0.00  25431519.00\n",
      "2  2025-08-07 09:15:01 AM  386.30      0         0.00  25431519.00\n",
      "3  2025-08-07 09:15:01 AM  386.30    895    345738.50  25777257.50\n",
      "4  2025-08-07 09:15:01 AM  386.75   1401    541836.75  26319094.25\n",
      "5  2025-08-07 09:15:02 AM  386.80   1795    694306.00  27013400.25\n",
      "6  2025-08-07 09:15:02 AM  386.95    741    286729.95  27300130.20\n",
      "7  2025-08-07 09:15:03 AM  386.85      0         0.00  27300130.20\n",
      "8  2025-08-07 09:15:03 AM  386.50      0         0.00  27300130.20\n",
      "9  2025-08-07 09:15:03 AM  386.90   2717   1051207.30  28351337.50\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== LAST 10 ROWS ===\n",
      "                         date   price   qty       trnvr     cum_trnvr\n",
      "18240  2025-08-07 03:29:31 PM  388.25   226    87744.50  4.446928e+09\n",
      "18241  2025-08-07 03:29:31 PM  388.10     0        0.00  4.446928e+09\n",
      "18242  2025-08-07 03:29:37 PM  388.25  2075   805618.75  4.447734e+09\n",
      "18243  2025-08-07 03:29:39 PM  388.30  1157   449263.10  4.448183e+09\n",
      "18244  2025-08-07 03:29:41 PM  388.10     0        0.00  4.448183e+09\n",
      "18245  2025-08-07 03:29:44 PM  388.25  1183   459299.75  4.448643e+09\n",
      "18246  2025-08-07 03:29:46 PM  388.25   958   371943.50  4.449015e+09\n",
      "18247  2025-08-07 03:29:51 PM  388.25  5331  2069760.75  4.451084e+09\n",
      "18248  2025-08-07 03:29:52 PM  388.25   539   209266.75  4.451294e+09\n",
      "18249  2025-08-07 03:29:58 PM  388.25  1740   675555.00  4.451969e+09\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== CHRONOLOGICAL ORDER CHECK ===\n",
      "First timestamp: 2025-08-07 09:15:00\n",
      "Last timestamp: 2025-08-07 15:29:58\n",
      "✓ Data is in CHRONOLOGICAL order (ascending)\n",
      "  - First row: Earliest time\n",
      "  - Last row: Latest time\n",
      "\n",
      "Total time range: 0 days 06:14:58\n"
     ]
    }
   ],
   "source": [
    "# Preview first and last 10 rows to check ordering\n",
    "print(\"=== FIRST 10 ROWS ===\")\n",
    "print(df.head(10))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"=== LAST 10 ROWS ===\")\n",
    "print(df.tail(10))\n",
    "\n",
    "# Check if data is in chronological order\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"=== CHRONOLOGICAL ORDER CHECK ===\")\n",
    "\n",
    "# Convert date column to datetime if not already\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Check first and last timestamps\n",
    "first_time = df['date'].iloc[0]\n",
    "last_time = df['date'].iloc[-1]\n",
    "\n",
    "print(f\"First timestamp: {first_time}\")\n",
    "print(f\"Last timestamp: {last_time}\")\n",
    "\n",
    "# Check if chronological (ascending) or reverse chronological (descending)\n",
    "if first_time < last_time:\n",
    "    print(\"✓ Data is in CHRONOLOGICAL order (ascending)\")\n",
    "    print(\"  - First row: Earliest time\")\n",
    "    print(\"  - Last row: Latest time\")\n",
    "else:\n",
    "    print(\"✗ Data is in REVERSE CHRONOLOGICAL order (descending)\")\n",
    "    print(\"  - First row: Latest time\")\n",
    "    print(\"  - Last row: Earliest time\")\n",
    "\n",
    "# Show time range\n",
    "time_range = last_time - first_time\n",
    "print(f\"\\nTotal time range: {time_range}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17543f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATETIME CONVERSION AND FEATURE EXTRACTION ===\n",
      "New datetime features added:\n",
      "  - date_only: object\n",
      "  - time: object\n",
      "  - hour: int32\n",
      "  - minute: int32\n",
      "  - second: int32\n",
      "\n",
      "=== SAMPLE DATA WITH NEW FEATURES ===\n",
      "                 date   date_only      time  hour  minute  second   price  \\\n",
      "0 2025-08-07 09:15:00  2025-08-07  09:15:00     9      15       0  386.85   \n",
      "1 2025-08-07 09:15:01  2025-08-07  09:15:01     9      15       1  386.65   \n",
      "2 2025-08-07 09:15:01  2025-08-07  09:15:01     9      15       1  386.30   \n",
      "3 2025-08-07 09:15:01  2025-08-07  09:15:01     9      15       1  386.30   \n",
      "4 2025-08-07 09:15:01  2025-08-07  09:15:01     9      15       1  386.75   \n",
      "5 2025-08-07 09:15:02  2025-08-07  09:15:02     9      15       2  386.80   \n",
      "6 2025-08-07 09:15:02  2025-08-07  09:15:02     9      15       2  386.95   \n",
      "7 2025-08-07 09:15:03  2025-08-07  09:15:03     9      15       3  386.85   \n",
      "8 2025-08-07 09:15:03  2025-08-07  09:15:03     9      15       3  386.50   \n",
      "9 2025-08-07 09:15:03  2025-08-07  09:15:03     9      15       3  386.90   \n",
      "\n",
      "     qty  \n",
      "0  65740  \n",
      "1      0  \n",
      "2      0  \n",
      "3    895  \n",
      "4   1401  \n",
      "5   1795  \n",
      "6    741  \n",
      "7      0  \n",
      "8      0  \n",
      "9   2717  \n",
      "\n",
      "=== DATETIME VERIFICATION ===\n",
      "Original date column dtype: datetime64[ns]\n",
      "First timestamp: 2025-08-07 09:15:00\n",
      "Last timestamp: 2025-08-07 15:29:58\n",
      "Total unique dates: 1\n",
      "Date range: 2025-08-07 to 2025-08-07\n"
     ]
    }
   ],
   "source": [
    "# Convert date to datetime64[ns] and extract datetime features\n",
    "print(\"=== DATETIME CONVERSION AND FEATURE EXTRACTION ===\")\n",
    "\n",
    "# Convert date column to datetime64[ns]\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Extract additional datetime features\n",
    "df['date_only'] = df['date'].dt.date\n",
    "df['time'] = df['date'].dt.time\n",
    "df['hour'] = df['date'].dt.hour\n",
    "df['minute'] = df['date'].dt.minute\n",
    "df['second'] = df['date'].dt.second\n",
    "\n",
    "# Display the new datetime features\n",
    "print(\"New datetime features added:\")\n",
    "print(f\"  - date_only: {df['date_only'].dtype}\")\n",
    "print(f\"  - time: {df['time'].dtype}\")\n",
    "print(f\"  - hour: {df['hour'].dtype}\")\n",
    "print(f\"  - minute: {df['minute'].dtype}\")\n",
    "print(f\"  - second: {df['second'].dtype}\")\n",
    "\n",
    "# Show sample of the enhanced dataframe\n",
    "print(\"\\n=== SAMPLE DATA WITH NEW FEATURES ===\")\n",
    "print(df[['date', 'date_only', 'time', 'hour', 'minute', 'second', 'price', 'qty']].head(10))\n",
    "\n",
    "# Verify datetime conversion\n",
    "print(f\"\\n=== DATETIME VERIFICATION ===\")\n",
    "print(f\"Original date column dtype: {df['date'].dtype}\")\n",
    "print(f\"First timestamp: {df['date'].iloc[0]}\")\n",
    "print(f\"Last timestamp: {df['date'].iloc[-1]}\")\n",
    "print(f\"Total unique dates: {df['date_only'].nunique()}\")\n",
    "print(f\"Date range: {df['date_only'].min()} to {df['date_only'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2229a48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HANDLING ZERO-QUANTITY TRADES ===\n",
      "Original data shape: (18250, 10)\n",
      "Rows with zero quantity: 7294\n",
      "Percentage of zero qty rows: 39.97%\n",
      "\n",
      "=== SAMPLE OF ZERO QTY ROWS (BEFORE REMOVAL) ===\n",
      "                  date   price  qty  trnvr    cum_trnvr\n",
      "1  2025-08-07 09:15:01  386.65    0    0.0  25431519.00\n",
      "2  2025-08-07 09:15:01  386.30    0    0.0  25431519.00\n",
      "7  2025-08-07 09:15:03  386.85    0    0.0  27300130.20\n",
      "8  2025-08-07 09:15:03  386.50    0    0.0  27300130.20\n",
      "13 2025-08-07 09:15:05  386.45    0    0.0  32995767.85\n",
      "\n",
      "=== AFTER CLEANING ===\n",
      "Cleaned data shape: (10956, 10)\n",
      "Rows removed: 7294\n",
      "Remaining rows: 10956\n",
      "\n",
      "=== SAMPLE OF CLEANED DATA ===\n",
      "                 date   price    qty        trnvr    cum_trnvr\n",
      "0 2025-08-07 09:15:00  386.85  65740  25431519.00  25431519.00\n",
      "1 2025-08-07 09:15:01  386.30    895    345738.50  25777257.50\n",
      "2 2025-08-07 09:15:01  386.75   1401    541836.75  26319094.25\n",
      "3 2025-08-07 09:15:02  386.80   1795    694306.00  27013400.25\n",
      "4 2025-08-07 09:15:02  386.95    741    286729.95  27300130.20\n",
      "5 2025-08-07 09:15:03  386.90   2717   1051207.30  28351337.50\n",
      "6 2025-08-07 09:15:03  386.80   9068   3507502.40  31858839.90\n",
      "7 2025-08-07 09:15:04  386.75   1141    441281.75  32300121.65\n",
      "8 2025-08-07 09:15:04  386.90   1798    695646.20  32995767.85\n",
      "9 2025-08-07 09:15:05  386.65   2092    808871.80  33804639.65\n",
      "\n",
      "Zero qty rows remaining: 0\n",
      "\n",
      "✓ Main dataframe 'df' now contains 10956 rows with non-zero quantities\n"
     ]
    }
   ],
   "source": [
    "# Handle zero-quantity trades - remove rows with 0 qty\n",
    "print(\"=== HANDLING ZERO-QUANTITY TRADES ===\")\n",
    "\n",
    "# Check current data shape and zero qty count\n",
    "print(f\"Original data shape: {df.shape}\")\n",
    "zero_qty_count = (df['qty'] == 0).sum()\n",
    "print(f\"Rows with zero quantity: {zero_qty_count}\")\n",
    "print(f\"Percentage of zero qty rows: {(zero_qty_count/len(df)*100):.2f}%\")\n",
    "\n",
    "# Show sample of zero qty rows before removal\n",
    "print(\"\\n=== SAMPLE OF ZERO QTY ROWS (BEFORE REMOVAL) ===\")\n",
    "zero_qty_sample = df[df['qty'] == 0][['date', 'price', 'qty', 'trnvr', 'cum_trnvr']].head(5)\n",
    "print(zero_qty_sample)\n",
    "\n",
    "# Remove rows with zero quantity\n",
    "df_clean = df[df['qty'] > 0].copy()\n",
    "\n",
    "# Reset index after filtering\n",
    "df_clean = df_clean.reset_index(drop=True)\n",
    "\n",
    "# Display results after cleaning\n",
    "print(f\"\\n=== AFTER CLEANING ===\")\n",
    "print(f\"Cleaned data shape: {df_clean.shape}\")\n",
    "print(f\"Rows removed: {len(df) - len(df_clean)}\")\n",
    "print(f\"Remaining rows: {len(df_clean)}\")\n",
    "\n",
    "# Show sample of cleaned data\n",
    "print(\"\\n=== SAMPLE OF CLEANED DATA ===\")\n",
    "print(df_clean[['date', 'price', 'qty', 'trnvr', 'cum_trnvr']].head(10))\n",
    "\n",
    "# Verify no zero qty rows remain\n",
    "remaining_zero_qty = (df_clean['qty'] == 0).sum()\n",
    "print(f\"\\nZero qty rows remaining: {remaining_zero_qty}\")\n",
    "\n",
    "# Update the main dataframe reference\n",
    "df = df_clean\n",
    "print(f\"\\n✓ Main dataframe 'df' now contains {len(df)} rows with non-zero quantities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b73b099b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NUMERIC COLUMN VALIDATION ===\n",
      "=== NEGATIVE VALUE CHECK ===\n",
      "price: 0 negative values\n",
      "qty: 0 negative values\n",
      "trnvr: 0 negative values\n",
      "cum_trnvr: 0 negative values\n",
      "\n",
      "=== ZERO VALUE CHECK ===\n",
      "price: 0 zero values\n",
      "qty: 0 zero values\n",
      "trnvr: 0 zero values\n",
      "cum_trnvr: 0 zero values\n",
      "\n",
      "=== STATISTICAL SUMMARY ===\n",
      "              price            qty         trnvr     cum_trnvr\n",
      "count  10956.000000   10956.000000  1.095600e+04  1.095600e+04\n",
      "mean     386.561094    1050.581508  4.063499e+05  2.377823e+09\n",
      "std        1.574324    3357.505716  1.299189e+06  1.165181e+09\n",
      "min      383.500000       1.000000  3.836000e+02  2.543152e+07\n",
      "25%      385.150000      26.000000  1.003177e+04  1.490814e+09\n",
      "50%      386.450000     200.000000  7.732500e+04  2.355502e+09\n",
      "75%      387.950000     799.000000  3.085895e+05  3.321858e+09\n",
      "max      390.200000  153858.000000  5.955074e+07  4.451969e+09\n",
      "\n",
      "=== OUTLIER DETECTION (IQR METHOD) ===\n",
      "\n",
      "price:\n",
      "  Q1: 385.15, Q3: 387.95, IQR: 2.80\n",
      "  Lower bound: 380.95, Upper bound: 392.15\n",
      "  Outliers below lower bound: 0\n",
      "  Outliers above upper bound: 0\n",
      "\n",
      "qty:\n",
      "  Q1: 26.00, Q3: 799.00, IQR: 773.00\n",
      "  Lower bound: -1133.50, Upper bound: 1958.50\n",
      "  Outliers below lower bound: 0\n",
      "  Outliers above upper bound: 1337\n",
      "\n",
      "trnvr:\n",
      "  Q1: 10031.77, Q3: 308589.53, IQR: 298557.75\n",
      "  Lower bound: -437804.85, Upper bound: 756426.15\n",
      "  Outliers below lower bound: 0\n",
      "  Outliers above upper bound: 1340\n",
      "\n",
      "cum_trnvr:\n",
      "  Q1: 1490814230.85, Q3: 3321857538.50, IQR: 1831043307.65\n",
      "  Lower bound: -1255750730.62, Upper bound: 6068422499.97\n",
      "  Outliers below lower bound: 0\n",
      "  Outliers above upper bound: 0\n",
      "\n",
      "=== EXTREME VALUE CHECK (3 STD DEV) ===\n",
      "\n",
      "price:\n",
      "  Mean: 386.56, Std: 1.57\n",
      "  Lower 3σ: 381.84, Upper 3σ: 391.28\n",
      "  Extreme values below: 0\n",
      "  Extreme values above: 0\n",
      "\n",
      "qty:\n",
      "  Mean: 1050.58, Std: 3357.51\n",
      "  Lower 3σ: -9021.94, Upper 3σ: 11123.10\n",
      "  Extreme values below: 0\n",
      "  Extreme values above: 148\n",
      "\n",
      "trnvr:\n",
      "  Mean: 406349.86, Std: 1299188.95\n",
      "  Lower 3σ: -3491216.99, Upper 3σ: 4303916.71\n",
      "  Extreme values below: 0\n",
      "  Extreme values above: 149\n",
      "\n",
      "cum_trnvr:\n",
      "  Mean: 2377822586.83, Std: 1165180692.88\n",
      "  Lower 3σ: -1117719491.81, Upper 3σ: 5873364665.46\n",
      "  Extreme values below: 0\n",
      "  Extreme values above: 0\n",
      "\n",
      "=== SAMPLE OF POTENTIAL OUTLIERS ===\n",
      "\n",
      "qty outliers (top 5):\n",
      "                  date    qty    qty        trnvr\n",
      "0  2025-08-07 09:15:00  65740  65740  25431519.00\n",
      "5  2025-08-07 09:15:03   2717   2717   1051207.30\n",
      "6  2025-08-07 09:15:03   9068   9068   3507502.40\n",
      "9  2025-08-07 09:15:05   2092   2092    808871.80\n",
      "11 2025-08-07 09:15:06   4519   4519   1747271.35\n",
      "\n",
      "trnvr outliers (top 5):\n",
      "                  date        trnvr    qty        trnvr\n",
      "0  2025-08-07 09:15:00  25431519.00  65740  25431519.00\n",
      "5  2025-08-07 09:15:03   1051207.30   2717   1051207.30\n",
      "6  2025-08-07 09:15:03   3507502.40   9068   3507502.40\n",
      "9  2025-08-07 09:15:05    808871.80   2092    808871.80\n",
      "11 2025-08-07 09:15:06   1747271.35   4519   1747271.35\n",
      "\n",
      "=== DATA QUALITY SUMMARY ===\n",
      "Total rows: 10956\n",
      "Columns with potential issues:\n",
      "  price: ✓ clean\n",
      "  qty: ✓ clean\n",
      "  trnvr: ✓ clean\n",
      "  cum_trnvr: ✓ clean\n"
     ]
    }
   ],
   "source": [
    "# Validate numeric columns for negatives or outliers\n",
    "print(\"=== NUMERIC COLUMN VALIDATION ===\")\n",
    "\n",
    "# List of numeric columns to validate\n",
    "numeric_cols = ['price', 'qty', 'trnvr', 'cum_trnvr']\n",
    "\n",
    "# Check for negative values\n",
    "print(\"=== NEGATIVE VALUE CHECK ===\")\n",
    "for col in numeric_cols:\n",
    "    negative_count = (df[col] < 0).sum()\n",
    "    print(f\"{col}: {negative_count} negative values\")\n",
    "\n",
    "# Check for zero values (after qty cleaning)\n",
    "print(\"\\n=== ZERO VALUE CHECK ===\")\n",
    "for col in numeric_cols:\n",
    "    zero_count = (df[col] == 0).sum()\n",
    "    print(f\"{col}: {zero_count} zero values\")\n",
    "\n",
    "# Statistical summary for outlier detection\n",
    "print(\"\\n=== STATISTICAL SUMMARY ===\")\n",
    "print(df[numeric_cols].describe())\n",
    "\n",
    "# Outlier detection using IQR method\n",
    "print(\"\\n=== OUTLIER DETECTION (IQR METHOD) ===\")\n",
    "for col in numeric_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers_lower = (df[col] < lower_bound).sum()\n",
    "    outliers_upper = (df[col] > upper_bound).sum()\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Q1: {Q1:.2f}, Q3: {Q3:.2f}, IQR: {IQR:.2f}\")\n",
    "    print(f\"  Lower bound: {lower_bound:.2f}, Upper bound: {upper_bound:.2f}\")\n",
    "    print(f\"  Outliers below lower bound: {outliers_lower}\")\n",
    "    print(f\"  Outliers above upper bound: {outliers_upper}\")\n",
    "\n",
    "# Check for extreme values (beyond 3 standard deviations)\n",
    "print(\"\\n=== EXTREME VALUE CHECK (3 STD DEV) ===\")\n",
    "for col in numeric_cols:\n",
    "    mean_val = df[col].mean()\n",
    "    std_val = df[col].std()\n",
    "    \n",
    "    lower_3std = mean_val - 3 * std_val\n",
    "    upper_3std = mean_val + 3 * std_val\n",
    "    \n",
    "    extreme_lower = (df[col] < lower_3std).sum()\n",
    "    extreme_upper = (df[col] > upper_3std).sum()\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Mean: {mean_val:.2f}, Std: {std_val:.2f}\")\n",
    "    print(f\"  Lower 3σ: {lower_3std:.2f}, Upper 3σ: {upper_3std:.2f}\")\n",
    "    print(f\"  Extreme values below: {extreme_lower}\")\n",
    "    print(f\"  Extreme values above: {extreme_upper}\")\n",
    "\n",
    "# Show sample of potential outliers\n",
    "print(\"\\n=== SAMPLE OF POTENTIAL OUTLIERS ===\")\n",
    "for col in numeric_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[df[col] > upper_bound]\n",
    "    if len(outliers) > 0:\n",
    "        print(f\"\\n{col} outliers (top 5):\")\n",
    "        print(outliers[['date', col, 'qty', 'trnvr']].head())\n",
    "\n",
    "# Data quality summary\n",
    "print(\"\\n=== DATA QUALITY SUMMARY ===\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Columns with potential issues:\")\n",
    "for col in numeric_cols:\n",
    "    issues = []\n",
    "    if (df[col] < 0).any():\n",
    "        issues.append(\"negative values\")\n",
    "    if (df[col] == 0).any() and col != 'qty':  # qty can legitimately be 0\n",
    "        issues.append(\"zero values\")\n",
    "    \n",
    "    if issues:\n",
    "        print(f\"  {col}: {', '.join(issues)}\")\n",
    "    else:\n",
    "        print(f\"  {col}: ✓ clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32fe8c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TIME-SERIES INTEGRITY CHECK AND SORTING ===\n",
      "=== CURRENT SORTING STATUS ===\n",
      "First timestamp: 2025-08-07 09:15:00\n",
      "Last timestamp: 2025-08-07 15:29:58\n",
      "Data is already sorted chronologically: True\n",
      "Duplicate timestamps: 1961\n",
      "\n",
      "=== DUPLICATE TIMESTAMP ANALYSIS ===\n",
      "Sample duplicate timestamps:\n",
      "                  date   price   qty       trnvr\n",
      "1  2025-08-07 09:15:01  386.30   895   345738.50\n",
      "2  2025-08-07 09:15:01  386.75  1401   541836.75\n",
      "3  2025-08-07 09:15:02  386.80  1795   694306.00\n",
      "4  2025-08-07 09:15:02  386.95   741   286729.95\n",
      "5  2025-08-07 09:15:03  386.90  2717  1051207.30\n",
      "6  2025-08-07 09:15:03  386.80  9068  3507502.40\n",
      "7  2025-08-07 09:15:04  386.75  1141   441281.75\n",
      "8  2025-08-07 09:15:04  386.90  1798   695646.20\n",
      "9  2025-08-07 09:15:05  386.65  2092   808871.80\n",
      "10 2025-08-07 09:15:05  386.75  1679   649353.25\n",
      "\n",
      "=== SORTING DATA BY DATETIME ===\n",
      "Data is now sorted chronologically: True\n",
      "\n",
      "=== SORTING VERIFICATION ===\n",
      "First 5 rows after sorting:\n",
      "                 date   price    qty        trnvr\n",
      "0 2025-08-07 09:15:00  386.85  65740  25431519.00\n",
      "1 2025-08-07 09:15:01  386.30    895    345738.50\n",
      "2 2025-08-07 09:15:01  386.75   1401    541836.75\n",
      "3 2025-08-07 09:15:02  386.80   1795    694306.00\n",
      "4 2025-08-07 09:15:02  386.95    741    286729.95\n",
      "\n",
      "Last 5 rows after sorting:\n",
      "                     date   price   qty       trnvr\n",
      "10951 2025-08-07 15:29:44  388.25  1183   459299.75\n",
      "10952 2025-08-07 15:29:46  388.25   958   371943.50\n",
      "10953 2025-08-07 15:29:51  388.25  5331  2069760.75\n",
      "10954 2025-08-07 15:29:52  388.25   539   209266.75\n",
      "10955 2025-08-07 15:29:58  388.25  1740   675555.00\n",
      "\n",
      "=== TIME SERIES CONTINUITY CHECK ===\n",
      "Time differences between consecutive rows:\n",
      "  Min: 0 days 00:00:00\n",
      "  Max: 0 days 00:00:12\n",
      "  Mean: 0 days 00:00:02.053674121\n",
      "  Most common: 0 days 00:00:01\n",
      "\n",
      "✓ Main dataframe 'df' is now properly sorted chronologically\n",
      "✓ Total rows: 10956\n",
      "✓ Time range: 2025-08-07 09:15:00 to 2025-08-07 15:29:58\n",
      "\n",
      "=== FINAL VERIFICATION ===\n",
      "✓ Data is sorted chronologically\n",
      "✓ Index is reset and sequential\n",
      "✓ Ready for time-series analysis\n"
     ]
    }
   ],
   "source": [
    "# Ensure sorting by datetime for time-series integrity\n",
    "print(\"=== TIME-SERIES INTEGRITY CHECK AND SORTING ===\")\n",
    "\n",
    "# Check current sorting status\n",
    "print(\"=== CURRENT SORTING STATUS ===\")\n",
    "print(f\"First timestamp: {df['date'].iloc[0]}\")\n",
    "print(f\"Last timestamp: {df['date'].iloc[-1]}\")\n",
    "\n",
    "# Check if data is already sorted\n",
    "is_sorted = df['date'].is_monotonic_increasing\n",
    "print(f\"Data is already sorted chronologically: {is_sorted}\")\n",
    "\n",
    "# Check for any duplicate timestamps\n",
    "duplicate_timestamps = df['date'].duplicated().sum()\n",
    "print(f\"Duplicate timestamps: {duplicate_timestamps}\")\n",
    "\n",
    "if duplicate_timestamps > 0:\n",
    "    print(\"\\n=== DUPLICATE TIMESTAMP ANALYSIS ===\")\n",
    "    duplicate_samples = df[df['date'].duplicated(keep=False)].sort_values('date')\n",
    "    print(\"Sample duplicate timestamps:\")\n",
    "    print(duplicate_samples[['date', 'price', 'qty', 'trnvr']].head(10))\n",
    "\n",
    "# Sort the dataframe by datetime\n",
    "print(\"\\n=== SORTING DATA BY DATETIME ===\")\n",
    "df_sorted = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Verify sorting\n",
    "is_now_sorted = df_sorted['date'].is_monotonic_increasing\n",
    "print(f\"Data is now sorted chronologically: {is_now_sorted}\")\n",
    "\n",
    "# Display sorting verification\n",
    "print(f\"\\n=== SORTING VERIFICATION ===\")\n",
    "print(\"First 5 rows after sorting:\")\n",
    "print(df_sorted[['date', 'price', 'qty', 'trnvr']].head())\n",
    "print(f\"\\nLast 5 rows after sorting:\")\n",
    "print(df_sorted[['date', 'price', 'qty', 'trnvr']].tail())\n",
    "\n",
    "# Check for any time gaps or irregularities\n",
    "print(f\"\\n=== TIME SERIES CONTINUITY CHECK ===\")\n",
    "time_diffs = df_sorted['date'].diff().dropna()\n",
    "print(f\"Time differences between consecutive rows:\")\n",
    "print(f\"  Min: {time_diffs.min()}\")\n",
    "print(f\"  Max: {time_diffs.max()}\")\n",
    "print(f\"  Mean: {time_diffs.mean()}\")\n",
    "print(f\"  Most common: {time_diffs.mode().iloc[0] if len(time_diffs.mode()) > 0 else 'N/A'}\")\n",
    "\n",
    "# Check for any large time gaps\n",
    "large_gaps = time_diffs[time_diffs > pd.Timedelta(minutes=5)]\n",
    "if len(large_gaps) > 0:\n",
    "    print(f\"\\n⚠️  Found {len(large_gaps)} time gaps larger than 5 minutes:\")\n",
    "    gap_indices = time_diffs[time_diffs > pd.Timedelta(minutes=5)].index\n",
    "    for idx in gap_indices[:5]:  # Show first 5 gaps\n",
    "        gap_start = df_sorted.loc[idx-1, 'date']\n",
    "        gap_end = df_sorted.loc[idx, 'date']\n",
    "        gap_duration = gap_end - gap_start\n",
    "        print(f\"  Gap: {gap_start} to {gap_end} (Duration: {gap_duration})\")\n",
    "\n",
    "# Update the main dataframe with sorted version\n",
    "df = df_sorted\n",
    "print(f\"\\n✓ Main dataframe 'df' is now properly sorted chronologically\")\n",
    "print(f\"✓ Total rows: {len(df)}\")\n",
    "print(f\"✓ Time range: {df['date'].min()} to {df['date'].max()}\")\n",
    "\n",
    "# Final verification\n",
    "print(f\"\\n=== FINAL VERIFICATION ===\")\n",
    "print(\"✓ Data is sorted chronologically\")\n",
    "print(\"✓ Index is reset and sequential\")\n",
    "print(\"✓ Ready for time-series analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f64545d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPREHENSIVE DESCRIPTIVE STATISTICS ===\n",
      "=== BASIC DESCRIPTIVE STATISTICS ===\n",
      "                                date         price            qty  \\\n",
      "count                          10956  10956.000000   10956.000000   \n",
      "mean   2025-08-07 12:40:58.929810176    386.561094    1050.581508   \n",
      "min              2025-08-07 09:15:00    383.500000       1.000000   \n",
      "25%       2025-08-07 11:06:26.500000    385.150000      26.000000   \n",
      "50%              2025-08-07 12:45:14    386.450000     200.000000   \n",
      "75%    2025-08-07 14:26:31.249999872    387.950000     799.000000   \n",
      "max              2025-08-07 15:29:58    390.200000  153858.000000   \n",
      "std                              NaN      1.574324    3357.505716   \n",
      "\n",
      "              trnvr     cum_trnvr          hour        minute        second  \n",
      "count  1.095600e+04  1.095600e+04  10956.000000  10956.000000  10956.000000  \n",
      "mean   4.063499e+05  2.377823e+09     12.209383     27.926798     29.543173  \n",
      "min    3.836000e+02  2.543152e+07      9.000000      0.000000      0.000000  \n",
      "25%    1.003177e+04  1.490814e+09     11.000000     13.000000     15.000000  \n",
      "50%    7.732500e+04  2.355502e+09     12.000000     26.000000     30.000000  \n",
      "75%    3.085895e+05  3.321858e+09     14.000000     43.000000     45.000000  \n",
      "max    5.955074e+07  4.451969e+09     15.000000     59.000000     59.000000  \n",
      "std    1.299189e+06  1.165181e+09      1.916078     17.252424     17.315855  \n",
      "\n",
      "============================================================\n",
      "=== DETAILED STATISTICS BY COLUMN ===\n",
      "\n",
      "--- PRICE ---\n",
      "Count: 10,956\n",
      "Mean: 386.56\n",
      "Std: 1.57\n",
      "Min: 383.50\n",
      "25%: 385.15\n",
      "50% (Median): 386.45\n",
      "75%: 387.95\n",
      "Max: 390.20\n",
      "Range: 6.70\n",
      "IQR: 2.80\n",
      "Coefficient of Variation: 0.41%\n",
      "\n",
      "--- QTY ---\n",
      "Count: 10,956\n",
      "Mean: 1,050.58\n",
      "Std: 3,357.51\n",
      "Min: 1.00\n",
      "25%: 26.00\n",
      "50% (Median): 200.00\n",
      "75%: 799.00\n",
      "Max: 153,858.00\n",
      "Range: 153,857.00\n",
      "IQR: 773.00\n",
      "Coefficient of Variation: 319.59%\n",
      "\n",
      "--- TRNVR ---\n",
      "Count: 10,956\n",
      "Mean: 406,349.86\n",
      "Std: 1,299,188.95\n",
      "Min: 383.60\n",
      "25%: 10,031.77\n",
      "50% (Median): 77,325.00\n",
      "75%: 308,589.53\n",
      "Max: 59,550,738.90\n",
      "Range: 59,550,355.30\n",
      "IQR: 298,557.75\n",
      "Coefficient of Variation: 319.72%\n",
      "\n",
      "--- CUM_TRNVR ---\n",
      "Count: 10,956\n",
      "Mean: 2,377,822,586.83\n",
      "Std: 1,165,180,692.88\n",
      "Min: 25,431,519.00\n",
      "25%: 1,490,814,230.85\n",
      "50% (Median): 2,355,501,798.67\n",
      "75%: 3,321,857,538.50\n",
      "Max: 4,451,969,111.70\n",
      "Range: 4,426,537,592.70\n",
      "IQR: 1,831,043,307.65\n",
      "Coefficient of Variation: 49.00%\n",
      "\n",
      "============================================================\n",
      "=== DATETIME FEATURE STATISTICS ===\n",
      "\n",
      "--- HOUR DISTRIBUTION ---\n",
      "hour\n",
      "9     1047\n",
      "10    1476\n",
      "11    1653\n",
      "12    1807\n",
      "13    1679\n",
      "14    1521\n",
      "15    1773\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- MINUTE DISTRIBUTION (Sample) ---\n",
      "minute\n",
      "0     165\n",
      "1     239\n",
      "2     221\n",
      "3     264\n",
      "4     115\n",
      "5     108\n",
      "6     111\n",
      "7     219\n",
      "8     280\n",
      "9     258\n",
      "10    271\n",
      "11    260\n",
      "12    114\n",
      "13    118\n",
      "14    120\n",
      "15    305\n",
      "16    208\n",
      "17    209\n",
      "18    228\n",
      "19    249\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "=== TIME PERIOD ANALYSIS ===\n",
      "Trades during market hours (9 AM - 3 PM): 10,956\n",
      "Trades outside market hours: 0\n",
      "\n",
      "--- PRICE ANALYSIS ---\n",
      "Price range: ₹383.50 to ₹390.20\n",
      "Price spread: ₹6.70\n",
      "\n",
      "--- VOLUME ANALYSIS ---\n",
      "Total volume traded: 11,510,171\n",
      "Average trade size: 1051\n",
      "Largest single trade: 153,858\n",
      "\n",
      "--- TURNOVER ANALYSIS ---\n",
      "Total turnover: ₹4,451,969,111.70\n",
      "Average trade value: ₹406,349.86\n",
      "Largest single trade value: ₹59,550,738.90\n",
      "\n",
      "============================================================\n",
      "=== SUMMARY TABLE ===\n",
      "        Metric                Value\n",
      "    Total Rows               10,956\n",
      "   Price Range    ₹383.50 - ₹390.20\n",
      "  Total Volume           11,510,171\n",
      "Total Turnover    ₹4,451,969,111.70\n",
      "    Time Range 09:15:00 to 15:29:58\n"
     ]
    }
   ],
   "source": [
    "# Get comprehensive descriptive statistics for numeric fields\n",
    "print(\"=== COMPREHENSIVE DESCRIPTIVE STATISTICS ===\")\n",
    "\n",
    "# Get basic describe() for all numeric columns\n",
    "print(\"=== BASIC DESCRIPTIVE STATISTICS ===\")\n",
    "print(df.describe())\n",
    "\n",
    "# Get detailed statistics for each numeric column\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== DETAILED STATISTICS BY COLUMN ===\")\n",
    "\n",
    "numeric_cols = ['price', 'qty', 'trnvr', 'cum_trnvr']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    print(f\"\\n--- {col.upper()} ---\")\n",
    "    col_stats = df[col].describe()\n",
    "    \n",
    "    print(f\"Count: {col_stats['count']:,.0f}\")\n",
    "    print(f\"Mean: {col_stats['mean']:,.2f}\")\n",
    "    print(f\"Std: {col_stats['std']:,.2f}\")\n",
    "    print(f\"Min: {col_stats['min']:,.2f}\")\n",
    "    print(f\"25%: {col_stats['25%']:,.2f}\")\n",
    "    print(f\"50% (Median): {col_stats['50%']:,.2f}\")\n",
    "    print(f\"75%: {col_stats['75%']:,.2f}\")\n",
    "    print(f\"Max: {col_stats['max']:,.2f}\")\n",
    "    \n",
    "    # Additional useful statistics\n",
    "    print(f\"Range: {col_stats['max'] - col_stats['min']:,.2f}\")\n",
    "    print(f\"IQR: {col_stats['75%'] - col_stats['25%']:,.2f}\")\n",
    "    print(f\"Coefficient of Variation: {(col_stats['std']/col_stats['mean']*100):,.2f}%\")\n",
    "\n",
    "# Get statistics for datetime features\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== DATETIME FEATURE STATISTICS ===\")\n",
    "\n",
    "print(\"\\n--- HOUR DISTRIBUTION ---\")\n",
    "hour_counts = df['hour'].value_counts().sort_index()\n",
    "print(hour_counts)\n",
    "\n",
    "print(\"\\n--- MINUTE DISTRIBUTION (Sample) ---\")\n",
    "minute_counts = df['minute'].value_counts().sort_index().head(20)\n",
    "print(minute_counts)\n",
    "\n",
    "# Get statistics for specific time periods\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== TIME PERIOD ANALYSIS ===\")\n",
    "\n",
    "# Market hours analysis (assuming 9:15 AM to 3:30 PM)\n",
    "market_hours = df[(df['hour'] >= 9) & (df['hour'] <= 15)]\n",
    "print(f\"Trades during market hours (9 AM - 3 PM): {len(market_hours):,}\")\n",
    "print(f\"Trades outside market hours: {len(df) - len(market_hours):,}\")\n",
    "\n",
    "# Price range analysis\n",
    "print(f\"\\n--- PRICE ANALYSIS ---\")\n",
    "print(f\"Price range: ₹{df['price'].min():.2f} to ₹{df['price'].max():.2f}\")\n",
    "print(f\"Price spread: ₹{df['price'].max() - df['price'].min():.2f}\")\n",
    "\n",
    "# Volume analysis\n",
    "print(f\"\\n--- VOLUME ANALYSIS ---\")\n",
    "print(f\"Total volume traded: {df['qty'].sum():,}\")\n",
    "print(f\"Average trade size: {df['qty'].mean():.0f}\")\n",
    "print(f\"Largest single trade: {df['qty'].max():,}\")\n",
    "\n",
    "# Turnover analysis\n",
    "print(f\"\\n--- TURNOVER ANALYSIS ---\")\n",
    "print(f\"Total turnover: ₹{df['trnvr'].sum():,.2f}\")\n",
    "print(f\"Average trade value: ₹{df['trnvr'].mean():,.2f}\")\n",
    "print(f\"Largest single trade value: ₹{df['trnvr'].max():,.2f}\")\n",
    "\n",
    "# Display summary table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== SUMMARY TABLE ===\")\n",
    "summary_data = {\n",
    "    'Metric': ['Total Rows', 'Price Range', 'Total Volume', 'Total Turnover', 'Time Range'],\n",
    "    'Value': [\n",
    "        f\"{len(df):,}\",\n",
    "        f\"₹{df['price'].min():.2f} - ₹{df['price'].max():.2f}\",\n",
    "        f\"{df['qty'].sum():,}\",\n",
    "        f\"₹{df['trnvr'].sum():,.2f}\",\n",
    "        f\"{df['date'].min().strftime('%H:%M:%S')} to {df['date'].max().strftime('%H:%M:%S')}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffcd6431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRADING DAYS ANALYSIS ===\n",
      "Total unique trading days: 1\n",
      "\n",
      "=== ALL TRADING DATES ===\n",
      " 1. 2025-08-07\n",
      "\n",
      "Trading date range: 2025-08-07 to 2025-08-07\n",
      "\n",
      "=== DATE ANALYSIS ===\n",
      "Single trading day data\n",
      "Date: 2025-08-07\n",
      "\n",
      "=== DATA CONSISTENCY CHECK ===\n",
      "Total rows in dataset: 10,956\n",
      "Average rows per trading day: 10956.0\n",
      "\n",
      "=== SUMMARY ===\n",
      "✓ Total unique trading days: 1\n",
      "✓ Date range: 2025-08-07 to 2025-08-07\n",
      "✓ Ready for daily analysis and aggregation\n"
     ]
    }
   ],
   "source": [
    "# Count total unique trading days\n",
    "print(\"=== TRADING DAYS ANALYSIS ===\")\n",
    "\n",
    "# Extract unique dates from the datetime column\n",
    "unique_dates = df['date_only'].unique()\n",
    "total_trading_days = len(unique_dates)\n",
    "\n",
    "print(f\"Total unique trading days: {total_trading_days}\")\n",
    "\n",
    "# Display all unique trading dates\n",
    "print(f\"\\n=== ALL TRADING DATES ===\")\n",
    "for i, date in enumerate(sorted(unique_dates), 1):\n",
    "    print(f\"{i:2d}. {date}\")\n",
    "\n",
    "# Get date range\n",
    "date_range = f\"{min(unique_dates)} to {max(unique_dates)}\"\n",
    "print(f\"\\nTrading date range: {date_range}\")\n",
    "\n",
    "# Check if all dates are from the same month/year\n",
    "print(f\"\\n=== DATE ANALYSIS ===\")\n",
    "if total_trading_days == 1:\n",
    "    print(\"Single trading day data\")\n",
    "    print(f\"Date: {unique_dates[0]}\")\n",
    "    # Initialize these variables for single day to avoid errors\n",
    "    months = {unique_dates[0].month}\n",
    "    years = {unique_dates[0].year}\n",
    "elif total_trading_days > 1:\n",
    "    # Check month and year consistency\n",
    "    months = set(date.month for date in unique_dates)\n",
    "    years = set(date.year for date in unique_dates)\n",
    "    \n",
    "    print(f\"Multiple trading days: {total_trading_days}\")\n",
    "    print(f\"Months covered: {sorted(months)}\")\n",
    "    print(f\"Years covered: {sorted(years)}\")\n",
    "    \n",
    "    if len(months) == 1:\n",
    "        month_name = pd.Timestamp(unique_dates[0]).strftime('%B')\n",
    "        print(f\"All dates are from: {month_name} {list(years)[0]}\")\n",
    "    \n",
    "    if len(years) == 1:\n",
    "        print(f\"All dates are from year: {list(years)[0]}\")\n",
    "\n",
    "# Trading days by month (if multiple months)\n",
    "if len(months) > 1:\n",
    "    print(f\"\\n=== TRADING DAYS BY MONTH ===\")\n",
    "    monthly_counts = {}\n",
    "    for date in unique_dates:\n",
    "        month_key = f\"{date.year}-{date.month:02d}\"\n",
    "        monthly_counts[month_key] = monthly_counts.get(month_key, 0) + 1\n",
    "    \n",
    "    for month_key in sorted(monthly_counts.keys()):\n",
    "        year, month = month_key.split('-')\n",
    "        month_name = pd.Timestamp(f\"{year}-{month}-01\").strftime('%B %Y')\n",
    "        print(f\"{month_name}: {monthly_counts[month_key]} trading days\")\n",
    "\n",
    "# Verify data consistency\n",
    "print(f\"\\n=== DATA CONSISTENCY CHECK ===\")\n",
    "print(f\"Total rows in dataset: {len(df):,}\")\n",
    "print(f\"Average rows per trading day: {len(df)/total_trading_days:.1f}\")\n",
    "\n",
    "# Check for any missing dates in sequence (if multiple days)\n",
    "if total_trading_days > 1:\n",
    "    sorted_dates = sorted(unique_dates)\n",
    "    date_diffs = []\n",
    "    for i in range(1, len(sorted_dates)):\n",
    "        diff = (sorted_dates[i] - sorted_dates[i-1]).days\n",
    "        date_diffs.append(diff)\n",
    "    \n",
    "    if any(diff > 1 for diff in date_diffs):\n",
    "        print(f\"\\n⚠️  Gaps detected in trading days:\")\n",
    "        for i, diff in enumerate(date_diffs):\n",
    "            if diff > 1:\n",
    "                gap_start = sorted_dates[i-1]\n",
    "                gap_end = sorted_dates[i]\n",
    "                print(f\"  Gap: {gap_start} to {gap_end} ({diff-1} missing days)\")\n",
    "    else:\n",
    "        print(f\"\\n✓ No gaps in trading days - consecutive trading days\")\n",
    "\n",
    "print(f\"\\n=== SUMMARY ===\")\n",
    "print(f\"✓ Total unique trading days: {total_trading_days}\")\n",
    "print(f\"✓ Date range: {date_range}\")\n",
    "print(f\"✓ Ready for daily analysis and aggregation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6881a59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TIMESTAMP RANGE ANALYSIS ===\n",
      "Earliest timestamp: 2025-08-07 09:15:00\n",
      "Latest timestamp: 2025-08-07 15:29:58\n",
      "Total time duration: 0 days 06:14:58\n",
      "Duration in hours: 6.25 hours\n",
      "Duration in minutes: 375 minutes\n",
      "\n",
      "=== EARLIEST TIMESTAMP ROW ===\n",
      "               date  price   qty      trnvr  cum_trnvr\n",
      "2025-08-07 09:15:00 386.85 65740 25431519.0 25431519.0\n",
      "\n",
      "=== LATEST TIMESTAMP ROW ===\n",
      "               date  price  qty    trnvr    cum_trnvr\n",
      "2025-08-07 15:29:58 388.25 1740 675555.0 4.451969e+09\n",
      "\n",
      "=== TIME PERIOD ANALYSIS ===\n",
      "Earliest: 09:15\n",
      "Latest: 15:29\n",
      "\n",
      "Market hours: 09:15:00 to 15:30:00\n",
      "Data coverage:\n",
      "\n",
      "=== TIME SERIES CONTINUITY ===\n",
      "Minimum time difference between consecutive rows: 0 days 00:00:00\n",
      "Maximum time difference between consecutive rows: 0 days 00:00:12\n",
      "\n",
      "=== SUMMARY ===\n",
      "✓ Data spans: 2025-08-07 09:15:00 to 2025-08-07 15:29:58\n",
      "✓ Total duration: 6.25 hours (375 minutes)\n",
      "✓ Total rows: 10,956\n",
      "✓ Average frequency: 1753.1 ticks per hour\n"
     ]
    }
   ],
   "source": [
    "# Identify earliest and latest timestamps\n",
    "print(\"=== TIMESTAMP RANGE ANALYSIS ===\")\n",
    "\n",
    "# Get earliest and latest timestamps\n",
    "earliest_timestamp = df['date'].min()\n",
    "latest_timestamp = df['date'].max()\n",
    "\n",
    "print(f\"Earliest timestamp: {earliest_timestamp}\")\n",
    "print(f\"Latest timestamp: {latest_timestamp}\")\n",
    "\n",
    "# Calculate total time duration\n",
    "total_duration = latest_timestamp - earliest_timestamp\n",
    "print(f\"Total time duration: {total_duration}\")\n",
    "\n",
    "# Convert duration to more readable format\n",
    "duration_hours = total_duration.total_seconds() / 3600\n",
    "duration_minutes = total_duration.total_seconds() / 60\n",
    "\n",
    "print(f\"Duration in hours: {duration_hours:.2f} hours\")\n",
    "print(f\"Duration in minutes: {duration_minutes:.0f} minutes\")\n",
    "\n",
    "# Display the actual rows with earliest and latest timestamps\n",
    "print(f\"\\n=== EARLIEST TIMESTAMP ROW ===\")\n",
    "earliest_row = df[df['date'] == earliest_timestamp]\n",
    "print(earliest_row[['date', 'price', 'qty', 'trnvr', 'cum_trnvr']].to_string(index=False))\n",
    "\n",
    "print(f\"\\n=== LATEST TIMESTAMP ROW ===\")\n",
    "latest_row = df[df['date'] == latest_timestamp]\n",
    "print(latest_row[['date', 'price', 'qty', 'trnvr', 'cum_trnvr']].to_string(index=False))\n",
    "\n",
    "# Check if timestamps span across different time periods\n",
    "print(f\"\\n=== TIME PERIOD ANALYSIS ===\")\n",
    "earliest_hour = earliest_timestamp.hour\n",
    "earliest_minute = earliest_timestamp.minute\n",
    "latest_hour = latest_timestamp.hour\n",
    "latest_minute = latest_timestamp.minute\n",
    "\n",
    "print(f\"Earliest: {earliest_hour:02d}:{earliest_minute:02d}\")\n",
    "print(f\"Latest: {latest_hour:02d}:{latest_minute:02d}\")\n",
    "\n",
    "# Market hours analysis (assuming 9:15 AM to 3:30 PM)\n",
    "market_start = pd.Timestamp('2025-08-07 09:15:00')\n",
    "market_end = pd.Timestamp('2025-08-07 15:30:00')\n",
    "\n",
    "print(f\"\\nMarket hours: 09:15:00 to 15:30:00\")\n",
    "print(f\"Data coverage:\")\n",
    "\n",
    "if earliest_timestamp < market_start:\n",
    "    pre_market_duration = market_start - earliest_timestamp\n",
    "    print(f\"  Pre-market: {earliest_timestamp.strftime('%H:%M:%S')} to {market_start.strftime('%H:%M:%S')} ({pre_market_duration})\")\n",
    "\n",
    "if earliest_timestamp <= market_start and latest_timestamp >= market_end:\n",
    "    market_duration = market_end - market_start\n",
    "    print(f\"  Market hours: {market_start.strftime('%H:%M:%S')} to {market_end.strftime('%H:%M:%S')} ({market_duration})\")\n",
    "\n",
    "if latest_timestamp > market_end:\n",
    "    post_market_duration = latest_timestamp - market_end\n",
    "    print(f\"  Post-market: {market_end.strftime('%H:%M:%S')} to {latest_timestamp.strftime('%H:%M:%S')} ({post_market_duration})\")\n",
    "\n",
    "# Check for any gaps in the time series\n",
    "print(f\"\\n=== TIME SERIES CONTINUITY ===\")\n",
    "time_diffs = df['date'].diff().dropna()\n",
    "min_time_diff = time_diffs.min()\n",
    "max_time_diff = time_diffs.max()\n",
    "\n",
    "print(f\"Minimum time difference between consecutive rows: {min_time_diff}\")\n",
    "print(f\"Maximum time difference between consecutive rows: {max_time_diff}\")\n",
    "\n",
    "# Identify any unusually large time gaps\n",
    "large_gaps = time_diffs[time_diffs > pd.Timedelta(minutes=1)]\n",
    "if len(large_gaps) > 0:\n",
    "    print(f\"\\n⚠️  Found {len(large_gaps)} time gaps larger than 1 minute:\")\n",
    "    gap_indices = large_gaps.index[:5]  # Show first 5 gaps\n",
    "    for idx in gap_indices:\n",
    "        gap_start = df.loc[idx-1, 'date']\n",
    "        gap_end = df.loc[idx, 'date']\n",
    "        gap_duration = gap_end - gap_start\n",
    "        print(f\"  Gap: {gap_start.strftime('%H:%M:%S')} to {gap_end.strftime('%H:%M:%S')} (Duration: {gap_duration})\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n=== SUMMARY ===\")\n",
    "print(f\"✓ Data spans: {earliest_timestamp.strftime('%Y-%m-%d %H:%M:%S')} to {latest_timestamp.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"✓ Total duration: {duration_hours:.2f} hours ({duration_minutes:.0f} minutes)\")\n",
    "print(f\"✓ Total rows: {len(df):,}\")\n",
    "print(f\"✓ Average frequency: {len(df)/duration_hours:.1f} ticks per hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18a43503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VOLUME AND TURNOVER ANALYSIS ===\n",
      "Total traded volume: 11,510,171\n",
      "Total turnover: ₹4,451,969,111.70\n",
      "\n",
      "=== DETAILED METRICS ===\n",
      "--- VOLUME ANALYSIS ---\n",
      "Average trade size: 1,051\n",
      "Median trade size: 200\n",
      "Largest single trade: 153,858\n",
      "Smallest single trade: 1\n",
      "\n",
      "--- TURNOVER ANALYSIS ---\n",
      "Average trade value: ₹406,349.86\n",
      "Median trade value: ₹77,325.00\n",
      "Largest single trade value: ₹59,550,738.90\n",
      "Smallest single trade value: ₹383.60\n",
      "\n",
      "--- PRICE ANALYSIS ---\n",
      "Simple average price: ₹386.56\n",
      "Weighted average price (by volume): ₹386.79\n",
      "Price range: ₹6.70\n",
      "\n",
      "--- VOLUME-WEIGHTED METRICS ---\n",
      "Volume Weighted Average Price (VWAP): ₹386.79\n",
      "\n",
      "--- EFFICIENCY METRICS ---\n",
      "Total number of trades: 10,956\n",
      "Average volume per trade: 1,051\n",
      "Average turnover per trade: ₹406,349.86\n",
      "\n",
      "=== TIME-BASED ANALYSIS ===\n",
      "Trading duration: 6.25 hours\n",
      "Volume per hour: 1,841,791\n",
      "Turnover per hour: ₹712,378,380.39\n",
      "\n",
      "--- MARKET ACTIVITY INTENSITY ---\n",
      "Trades per hour: 1753.1\n",
      "Volume per trade: 1,051\n",
      "Turnover per trade: ₹406,349.86\n",
      "\n",
      "============================================================\n",
      "=== SUMMARY TABLE ===\n",
      "         Metric             Value\n",
      "   Total Volume        11,510,171\n",
      " Total Turnover ₹4,451,969,111.70\n",
      "   Total Trades            10,956\n",
      " Avg Trade Size             1,051\n",
      "Avg Trade Value       ₹406,349.86\n",
      "           VWAP           ₹386.79\n",
      "\n",
      "=== KEY INSIGHTS ===\n",
      "✓ Total volume traded: 11,510,171 shares\n",
      "✓ Total market value: ₹4,451,969,111.70\n",
      "✓ Market activity: 10,956 individual trades\n",
      "✓ Trading efficiency: ₹386.79 per share\n"
     ]
    }
   ],
   "source": [
    "# Calculate total traded volume and total turnover\n",
    "print(\"=== VOLUME AND TURNOVER ANALYSIS ===\")\n",
    "\n",
    "# Calculate totals\n",
    "total_volume = df['qty'].sum()\n",
    "total_turnover = df['trnvr'].sum()\n",
    "\n",
    "print(f\"Total traded volume: {total_volume:,}\")\n",
    "print(f\"Total turnover: ₹{total_turnover:,.2f}\")\n",
    "\n",
    "# Additional volume and turnover metrics\n",
    "print(f\"\\n=== DETAILED METRICS ===\")\n",
    "\n",
    "# Volume analysis\n",
    "print(\"--- VOLUME ANALYSIS ---\")\n",
    "avg_trade_size = df['qty'].mean()\n",
    "median_trade_size = df['qty'].median()\n",
    "max_trade_size = df['qty'].max()\n",
    "min_trade_size = df['qty'].min()\n",
    "\n",
    "print(f\"Average trade size: {avg_trade_size:,.0f}\")\n",
    "print(f\"Median trade size: {median_trade_size:,.0f}\")\n",
    "print(f\"Largest single trade: {max_trade_size:,}\")\n",
    "print(f\"Smallest single trade: {min_trade_size:,}\")\n",
    "\n",
    "# Turnover analysis\n",
    "print(f\"\\n--- TURNOVER ANALYSIS ---\")\n",
    "avg_trade_value = df['trnvr'].mean()\n",
    "median_trade_value = df['trnvr'].median()\n",
    "max_trade_value = df['trnvr'].max()\n",
    "min_trade_value = df['trnvr'].min()\n",
    "\n",
    "print(f\"Average trade value: ₹{avg_trade_value:,.2f}\")\n",
    "print(f\"Median trade value: ₹{median_trade_value:,.2f}\")\n",
    "print(f\"Largest single trade value: ₹{max_trade_value:,.2f}\")\n",
    "print(f\"Smallest single trade value: ₹{min_trade_value:,.2f}\")\n",
    "\n",
    "# Price analysis\n",
    "print(f\"\\n--- PRICE ANALYSIS ---\")\n",
    "avg_price = df['price'].mean()\n",
    "weighted_avg_price = (df['price'] * df['qty']).sum() / df['qty'].sum()\n",
    "price_range = df['price'].max() - df['price'].min()\n",
    "\n",
    "print(f\"Simple average price: ₹{avg_price:.2f}\")\n",
    "print(f\"Weighted average price (by volume): ₹{weighted_avg_price:.2f}\")\n",
    "print(f\"Price range: ₹{price_range:.2f}\")\n",
    "\n",
    "# Volume-weighted metrics\n",
    "print(f\"\\n--- VOLUME-WEIGHTED METRICS ---\")\n",
    "vwap = (df['price'] * df['qty']).sum() / df['qty'].sum()\n",
    "print(f\"Volume Weighted Average Price (VWAP): ₹{vwap:.2f}\")\n",
    "\n",
    "# Efficiency metrics\n",
    "print(f\"\\n--- EFFICIENCY METRICS ---\")\n",
    "trades_count = len(df)\n",
    "print(f\"Total number of trades: {trades_count:,}\")\n",
    "print(f\"Average volume per trade: {total_volume/trades_count:,.0f}\")\n",
    "print(f\"Average turnover per trade: ₹{total_turnover/trades_count:,.2f}\")\n",
    "\n",
    "# Time-based analysis\n",
    "print(f\"\\n=== TIME-BASED ANALYSIS ===\")\n",
    "earliest_time = df['date'].min()\n",
    "latest_time = df['date'].max()\n",
    "duration_hours = (latest_time - earliest_time).total_seconds() / 3600\n",
    "\n",
    "print(f\"Trading duration: {duration_hours:.2f} hours\")\n",
    "print(f\"Volume per hour: {total_volume/duration_hours:,.0f}\")\n",
    "print(f\"Turnover per hour: ₹{total_turnover/duration_hours:,.2f}\")\n",
    "\n",
    "# Market activity intensity\n",
    "print(f\"\\n--- MARKET ACTIVITY INTENSITY ---\")\n",
    "print(f\"Trades per hour: {trades_count/duration_hours:.1f}\")\n",
    "print(f\"Volume per trade: {total_volume/trades_count:,.0f}\")\n",
    "print(f\"Turnover per trade: ₹{total_turnover/trades_count:,.2f}\")\n",
    "\n",
    "# Summary table\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"=== SUMMARY TABLE ===\")\n",
    "summary_data = {\n",
    "    'Metric': ['Total Volume', 'Total Turnover', 'Total Trades', 'Avg Trade Size', 'Avg Trade Value', 'VWAP'],\n",
    "    'Value': [\n",
    "        f\"{total_volume:,}\",\n",
    "        f\"₹{total_turnover:,.2f}\",\n",
    "        f\"{trades_count:,}\",\n",
    "        f\"{avg_trade_size:,.0f}\",\n",
    "        f\"₹{avg_trade_value:,.2f}\",\n",
    "        f\"₹{vwap:.2f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n=== KEY INSIGHTS ===\")\n",
    "print(f\"✓ Total volume traded: {total_volume:,} shares\")\n",
    "print(f\"✓ Total market value: ₹{total_turnover:,.2f}\")\n",
    "print(f\"✓ Market activity: {trades_count:,} individual trades\")\n",
    "print(f\"✓ Trading efficiency: ₹{total_turnover/total_volume:.2f} per share\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e17d8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRADE TYPE ANALYSIS ===\n",
      "Total records in dataset: 10,956\n",
      "Actual trades (qty > 0): 10,956\n",
      "Zero-quantity updates (qty = 0): 0\n",
      "\n",
      "=== PERCENTAGE BREAKDOWN ===\n",
      "Actual trades: 100.00%\n",
      "Zero-quantity updates: 0.00%\n",
      "\n",
      "=== DATA COMPOSITION ANALYSIS ===\n",
      "✓ Dataset contains only actual trades (all qty > 0)\n",
      "\n",
      "=== SAMPLE OF ACTUAL TRADES (qty > 0) ===\n",
      "                 date   price    qty        trnvr    cum_trnvr\n",
      "0 2025-08-07 09:15:00  386.85  65740  25431519.00  25431519.00\n",
      "1 2025-08-07 09:15:01  386.30    895    345738.50  25777257.50\n",
      "2 2025-08-07 09:15:01  386.75   1401    541836.75  26319094.25\n",
      "3 2025-08-07 09:15:02  386.80   1795    694306.00  27013400.25\n",
      "4 2025-08-07 09:15:02  386.95    741    286729.95  27300130.20\n",
      "5 2025-08-07 09:15:03  386.90   2717   1051207.30  28351337.50\n",
      "6 2025-08-07 09:15:03  386.80   9068   3507502.40  31858839.90\n",
      "7 2025-08-07 09:15:04  386.75   1141    441281.75  32300121.65\n",
      "8 2025-08-07 09:15:04  386.90   1798    695646.20  32995767.85\n",
      "9 2025-08-07 09:15:05  386.75   1679    649353.25  34453992.90\n",
      "\n",
      "=== CHARACTERISTICS ANALYSIS ===\n",
      "--- ACTUAL TRADES (qty > 0) ---\n",
      "  Total volume: 11,510,171\n",
      "  Total turnover: ₹4,451,969,111.70\n",
      "  Average trade size: 1,051\n",
      "  Average trade value: ₹406,349.86\n",
      "  Price range: ₹383.50 - ₹390.20\n",
      "\n",
      "=== DATA QUALITY IMPLICATIONS ===\n",
      "✓ Clean dataset with only actual trades\n",
      "\n",
      "=== RECOMMENDATIONS ===\n",
      "  ✓ Dataset is ready for all types of analysis\n",
      "\n",
      "=== SUMMARY ===\n",
      "✓ Total records: 10,956\n",
      "✓ Actual trades: 10,956 (100.0%)\n",
      "✓ Zero-qty updates: 0 (0.0%)\n",
      "✓ Data type: Pure trades only\n"
     ]
    }
   ],
   "source": [
    "# Check number of trades with qty > 0 (actual trades vs. zero-qty updates)\n",
    "print(\"=== TRADE TYPE ANALYSIS ===\")\n",
    "\n",
    "# Count different types of records\n",
    "total_records = len(df)\n",
    "actual_trades = (df['qty'] > 0).sum()\n",
    "zero_qty_updates = (df['qty'] == 0).sum()\n",
    "\n",
    "print(f\"Total records in dataset: {total_records:,}\")\n",
    "print(f\"Actual trades (qty > 0): {actual_trades:,}\")\n",
    "print(f\"Zero-quantity updates (qty = 0): {zero_qty_updates:,}\")\n",
    "\n",
    "# Calculate percentages\n",
    "actual_trades_pct = (actual_trades / total_records) * 100\n",
    "zero_qty_pct = (zero_qty_updates / total_records) * 100\n",
    "\n",
    "print(f\"\\n=== PERCENTAGE BREAKDOWN ===\")\n",
    "print(f\"Actual trades: {actual_trades_pct:.2f}%\")\n",
    "print(f\"Zero-quantity updates: {zero_qty_pct:.2f}%\")\n",
    "\n",
    "# Analyze the data composition\n",
    "print(f\"\\n=== DATA COMPOSITION ANALYSIS ===\")\n",
    "if zero_qty_updates > 0:\n",
    "    print(\"⚠️  Dataset contains both actual trades and zero-quantity updates\")\n",
    "    print(\"   This suggests the data includes bid-ask spread updates\")\n",
    "else:\n",
    "    print(\"✓ Dataset contains only actual trades (all qty > 0)\")\n",
    "\n",
    "# Show sample of actual trades\n",
    "print(f\"\\n=== SAMPLE OF ACTUAL TRADES (qty > 0) ===\")\n",
    "actual_trades_df = df[df['qty'] > 0]\n",
    "print(actual_trades_df[['date', 'price', 'qty', 'trnvr', 'cum_trnvr']].head(10))\n",
    "\n",
    "# Show sample of zero-quantity updates (if any exist)\n",
    "if zero_qty_updates > 0:\n",
    "    print(f\"\\n=== SAMPLE OF ZERO-QUANTITY UPDATES (qty = 0) ===\")\n",
    "    zero_qty_df = df[df['qty'] == 0]\n",
    "    print(zero_qty_df[['date', 'price', 'qty', 'trnvr', 'cum_trnvr']].head(10))\n",
    "\n",
    "# Analyze characteristics of each type\n",
    "print(f\"\\n=== CHARACTERISTICS ANALYSIS ===\")\n",
    "\n",
    "# Actual trades characteristics\n",
    "if actual_trades > 0:\n",
    "    print(\"--- ACTUAL TRADES (qty > 0) ---\")\n",
    "    actual_trades_data = df[df['qty'] > 0]\n",
    "    print(f\"  Total volume: {actual_trades_data['qty'].sum():,}\")\n",
    "    print(f\"  Total turnover: ₹{actual_trades_data['trnvr'].sum():,.2f}\")\n",
    "    print(f\"  Average trade size: {actual_trades_data['qty'].mean():,.0f}\")\n",
    "    print(f\"  Average trade value: ₹{actual_trades_data['trnvr'].mean():,.2f}\")\n",
    "    print(f\"  Price range: ₹{actual_trades_data['price'].min():.2f} - ₹{actual_trades_data['price'].max():.2f}\")\n",
    "\n",
    "# Zero-quantity updates characteristics (if any exist)\n",
    "if zero_qty_updates > 0:\n",
    "    print(f\"\\n--- ZERO-QUANTITY UPDATES (qty = 0) ---\")\n",
    "    zero_qty_data = df[df['qty'] == 0]\n",
    "    print(f\"  Total records: {len(zero_qty_data):,}\")\n",
    "    print(f\"  Price range: ₹{zero_qty_data['price'].min():.2f} - ₹{zero_qty_data['price'].max():.2f}\")\n",
    "    print(f\"  Average price: ₹{zero_qty_data['price'].mean():.2f}\")\n",
    "    \n",
    "    # Check if these are bid-ask spread updates\n",
    "    if zero_qty_data['trnvr'].sum() == 0:\n",
    "        print(f\"  All have zero turnover (typical bid-ask updates)\")\n",
    "    else:\n",
    "        print(f\"  Some have non-zero turnover (data quality issue)\")\n",
    "\n",
    "# Data quality implications\n",
    "print(f\"\\n=== DATA QUALITY IMPLICATIONS ===\")\n",
    "if zero_qty_updates > 0:\n",
    "    print(\"⚠️  Mixed data types detected:\")\n",
    "    print(\"   - Actual trades: Use for volume, turnover, and price analysis\")\n",
    "    print(\"   - Zero-qty updates: Use for bid-ask spread analysis only\")\n",
    "    print(\"   - Consider filtering by qty > 0 for trade-based analysis\")\n",
    "else:\n",
    "    print(\"✓ Clean dataset with only actual trades\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\n=== RECOMMENDATIONS ===\")\n",
    "if zero_qty_updates > 0:\n",
    "    print(\"For different types of analysis:\")\n",
    "    print(\"  📊 Volume/Turnover analysis: Use df[df['qty'] > 0]\")\n",
    "    print(\"  📈 Price movement analysis: Use df[df['qty'] > 0]\")\n",
    "    print(\"  🔍 Bid-ask spread analysis: Use df[df['qty'] == 0]\")\n",
    "    print(\"  📋 Complete market picture: Use full dataset\")\n",
    "else:\n",
    "    print(\"  ✓ Dataset is ready for all types of analysis\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n=== SUMMARY ===\")\n",
    "print(f\"✓ Total records: {total_records:,}\")\n",
    "print(f\"✓ Actual trades: {actual_trades:,} ({actual_trades_pct:.1f}%)\")\n",
    "print(f\"✓ Zero-qty updates: {zero_qty_updates:,} ({zero_qty_pct:.1f}%)\")\n",
    "print(f\"✓ Data type: {'Mixed (trades + updates)' if zero_qty_updates > 0 else 'Pure trades only'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01afc28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PRICE CHANGE CALCULATION ===\n",
      "=== SAMPLE DATA WITH PRICE CHANGE ===\n",
      "                 date   price  price_change    qty        trnvr\n",
      "0 2025-08-07 09:15:00  386.85           NaN  65740  25431519.00\n",
      "1 2025-08-07 09:15:01  386.30         -0.55    895    345738.50\n",
      "2 2025-08-07 09:15:01  386.75          0.45   1401    541836.75\n",
      "3 2025-08-07 09:15:02  386.80          0.05   1795    694306.00\n",
      "4 2025-08-07 09:15:02  386.95          0.15    741    286729.95\n",
      "5 2025-08-07 09:15:03  386.90         -0.05   2717   1051207.30\n",
      "6 2025-08-07 09:15:03  386.80         -0.10   9068   3507502.40\n",
      "7 2025-08-07 09:15:04  386.75         -0.05   1141    441281.75\n",
      "8 2025-08-07 09:15:04  386.90          0.15   1798    695646.20\n",
      "9 2025-08-07 09:15:05  386.75         -0.15   1679    649353.25\n",
      "\n",
      "=== PRICE CHANGE STATISTICS ===\n",
      "Total price changes calculated: 10955\n",
      "First price change: -0.55\n",
      "Last price change: 0.00\n",
      "\n",
      "=== PRICE CHANGE DESCRIPTIVE STATISTICS ===\n",
      "count    10955.000000\n",
      "mean         0.000128\n",
      "std          0.083893\n",
      "min         -0.650000\n",
      "25%         -0.050000\n",
      "50%          0.000000\n",
      "75%          0.050000\n",
      "max          0.750000\n",
      "Name: price_change, dtype: float64\n",
      "\n",
      "=== PRICE CHANGE DISTRIBUTION ANALYSIS ===\n",
      "Positive price changes: 3,216 (29.36%)\n",
      "Negative price changes: 3,299 (30.11%)\n",
      "No price changes: 4,440 (40.53%)\n",
      "\n",
      "=== PRICE CHANGE MAGNITUDE ANALYSIS ===\n",
      "Average absolute price change: ₹0.05\n",
      "Median absolute price change: ₹0.05\n",
      "Largest price increase: ₹0.75\n",
      "Largest price decrease: ₹-0.65\n",
      "\n",
      "=== EXAMPLES OF PRICE CHANGES ===\n",
      "Top 5 largest price increases:\n",
      "               date  price  price_change   qty\n",
      "2025-08-07 09:15:16 386.55          0.75  5645\n",
      "2025-08-07 09:18:55 389.05          0.60 17196\n",
      "2025-08-07 09:20:49 389.85          0.55 43873\n",
      "2025-08-07 09:15:11 386.20          0.50  1962\n",
      "2025-08-07 14:45:13 386.55          0.50   579\n",
      "\n",
      "Top 5 largest price decreases:\n",
      "               date  price  price_change   qty\n",
      "2025-08-07 09:18:47 388.45         -0.65 17426\n",
      "2025-08-07 09:15:01 386.30         -0.55   895\n",
      "2025-08-07 09:15:26 385.90         -0.45  2839\n",
      "2025-08-07 09:16:20 387.15         -0.45 11997\n",
      "2025-08-07 09:15:15 385.80         -0.45  3449\n",
      "\n",
      "Sample of rows with no price change:\n",
      "               date  price  price_change  qty\n",
      "2025-08-07 09:15:06 386.65           0.0 4519\n",
      "2025-08-07 09:15:09 386.00           0.0 1904\n",
      "2025-08-07 09:15:10 386.00           0.0 6328\n",
      "2025-08-07 09:15:12 386.00           0.0 1803\n",
      "2025-08-07 09:15:13 385.75           0.0 8471\n",
      "\n",
      "=== PRICE CHANGE PATTERNS ===\n",
      "Price changes per hour: 1753.0\n",
      "Average price change frequency: 1.00 changes per tick\n",
      "\n",
      "=== CALCULATION VERIFICATION ===\n",
      "✓ Price change column created successfully\n",
      "✓ First row price_change is NaN (no previous price to compare)\n",
      "✓ Total rows: 10,956\n",
      "✓ Price changes calculated: 10,955\n",
      "✓ Ready for price movement analysis\n",
      "\n",
      "=== UPDATED DATAFRAME INFO ===\n",
      "Columns: ['date', 'price', 'qty', 'trnvr', 'cum_trnvr', 'date_only', 'time', 'hour', 'minute', 'second', 'price_change']\n",
      "Shape: (10956, 11)\n",
      "Data types:\n",
      "date            datetime64[ns]\n",
      "price                  float64\n",
      "qty                      int64\n",
      "trnvr                  float64\n",
      "cum_trnvr              float64\n",
      "date_only               object\n",
      "time                    object\n",
      "hour                     int32\n",
      "minute                   int32\n",
      "second                   int32\n",
      "price_change           float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Create price_change = current price - previous price\n",
    "print(\"=== PRICE CHANGE CALCULATION ===\")\n",
    "\n",
    "# Calculate price change (current price - previous price)\n",
    "df['price_change'] = df['price'].diff()\n",
    "\n",
    "# Display the first few rows to verify the calculation\n",
    "print(\"=== SAMPLE DATA WITH PRICE CHANGE ===\")\n",
    "print(df[['date', 'price', 'price_change', 'qty', 'trnvr']].head(10))\n",
    "\n",
    "# Basic statistics of price changes\n",
    "print(f\"\\n=== PRICE CHANGE STATISTICS ===\")\n",
    "print(f\"Total price changes calculated: {len(df['price_change'].dropna())}\")\n",
    "print(f\"First price change: {df['price_change'].iloc[1]:.2f}\")  # First change is at index 1\n",
    "print(f\"Last price change: {df['price_change'].iloc[-1]:.2f}\")\n",
    "\n",
    "# Statistical summary of price changes\n",
    "print(f\"\\n=== PRICE CHANGE DESCRIPTIVE STATISTICS ===\")\n",
    "price_change_stats = df['price_change'].describe()\n",
    "print(price_change_stats)\n",
    "\n",
    "# Analyze price change distribution\n",
    "print(f\"\\n=== PRICE CHANGE DISTRIBUTION ANALYSIS ===\")\n",
    "positive_changes = (df['price_change'] > 0).sum()\n",
    "negative_changes = (df['price_change'] < 0).sum()\n",
    "zero_changes = (df['price_change'] == 0).sum()\n",
    "total_changes = len(df['price_change'].dropna())\n",
    "\n",
    "print(f\"Positive price changes: {positive_changes:,} ({(positive_changes/total_changes*100):.2f}%)\")\n",
    "print(f\"Negative price changes: {negative_changes:,} ({(negative_changes/total_changes*100):.2f}%)\")\n",
    "print(f\"No price changes: {zero_changes:,} ({(zero_changes/total_changes*100):.2f}%)\")\n",
    "\n",
    "# Price change magnitude analysis\n",
    "print(f\"\\n=== PRICE CHANGE MAGNITUDE ANALYSIS ===\")\n",
    "abs_price_changes = df['price_change'].abs()\n",
    "print(f\"Average absolute price change: ₹{abs_price_changes.mean():.2f}\")\n",
    "print(f\"Median absolute price change: ₹{abs_price_changes.median():.2f}\")\n",
    "print(f\"Largest price increase: ₹{df['price_change'].max():.2f}\")\n",
    "print(f\"Largest price decrease: ₹{df['price_change'].min():.2f}\")\n",
    "\n",
    "# Show examples of different types of price changes\n",
    "print(f\"\\n=== EXAMPLES OF PRICE CHANGES ===\")\n",
    "\n",
    "# Largest price increases\n",
    "print(\"Top 5 largest price increases:\")\n",
    "largest_increases = df.nlargest(5, 'price_change')[['date', 'price', 'price_change', 'qty']]\n",
    "print(largest_increases.to_string(index=False))\n",
    "\n",
    "# Largest price decreases\n",
    "print(f\"\\nTop 5 largest price decreases:\")\n",
    "largest_decreases = df.nsmallest(5, 'price_change')[['date', 'price', 'price_change', 'qty']]\n",
    "print(largest_decreases.to_string(index=False))\n",
    "\n",
    "# No change examples\n",
    "if zero_changes > 0:\n",
    "    print(f\"\\nSample of rows with no price change:\")\n",
    "    no_change_sample = df[df['price_change'] == 0][['date', 'price', 'price_change', 'qty']].head(5)\n",
    "    print(no_change_sample.to_string(index=False))\n",
    "\n",
    "# Price change patterns\n",
    "print(f\"\\n=== PRICE CHANGE PATTERNS ===\")\n",
    "print(f\"Price changes per hour: {total_changes / ((df['date'].max() - df['date'].min()).total_seconds() / 3600):.1f}\")\n",
    "print(f\"Average price change frequency: {total_changes / len(df):.2f} changes per tick\")\n",
    "\n",
    "# Verify calculation integrity\n",
    "print(f\"\\n=== CALCULATION VERIFICATION ===\")\n",
    "print(f\"✓ Price change column created successfully\")\n",
    "print(f\"✓ First row price_change is NaN (no previous price to compare)\")\n",
    "print(f\"✓ Total rows: {len(df):,}\")\n",
    "print(f\"✓ Price changes calculated: {total_changes:,}\")\n",
    "print(f\"✓ Ready for price movement analysis\")\n",
    "\n",
    "# Display final dataframe info\n",
    "print(f\"\\n=== UPDATED DATAFRAME INFO ===\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Data types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ee472b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DIRECTION COLUMN CREATION ===\n",
      "=== SAMPLE DATA WITH DIRECTION ===\n",
      "                 date   price  price_change  direction    qty        trnvr\n",
      "0 2025-08-07 09:15:00  386.85           NaN  No change  65740  25431519.00\n",
      "1 2025-08-07 09:15:01  386.30         -0.55       Down    895    345738.50\n",
      "2 2025-08-07 09:15:01  386.75          0.45         Up   1401    541836.75\n",
      "3 2025-08-07 09:15:02  386.80          0.05         Up   1795    694306.00\n",
      "4 2025-08-07 09:15:02  386.95          0.15         Up    741    286729.95\n",
      "5 2025-08-07 09:15:03  386.90         -0.05       Down   2717   1051207.30\n",
      "6 2025-08-07 09:15:03  386.80         -0.10       Down   9068   3507502.40\n",
      "7 2025-08-07 09:15:04  386.75         -0.05       Down   1141    441281.75\n",
      "8 2025-08-07 09:15:04  386.90          0.15         Up   1798    695646.20\n",
      "9 2025-08-07 09:15:05  386.75         -0.15       Down   1679    649353.25\n",
      "\n",
      "=== DIRECTION DISTRIBUTION ===\n",
      "direction\n",
      "No change    4441\n",
      "Down         3299\n",
      "Up           3216\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== DIRECTION PERCENTAGES ===\n",
      "No change: 4,441 (40.53%)\n",
      "Down: 3,299 (30.11%)\n",
      "Up: 3,216 (29.35%)\n",
      "\n",
      "=== DIRECTION PATTERN ANALYSIS ===\n",
      "Direction sequence analysis:\n",
      "Maximum consecutive 'Up' movements: 7\n",
      "Maximum consecutive 'Down' movements: 5\n",
      "Maximum consecutive 'No change': 13\n",
      "\n",
      "=== DIRECTION BY TIME PERIODS ===\n",
      "Direction distribution by hour:\n",
      "direction  Down  No change   Up\n",
      "hour                           \n",
      "9           348        363  336\n",
      "10          478        531  467\n",
      "11          538        630  485\n",
      "12          565        698  544\n",
      "13          473        751  455\n",
      "14          428        643  450\n",
      "15          469        825  479\n",
      "\n",
      "=== DIRECTION TRANSITIONS ===\n",
      "Most common direction transitions:\n",
      "  No change → No change: 2,073 times\n",
      "  Down → Up: 1,789 times\n",
      "  Up → Down: 1,434 times\n",
      "  No change → Down: 1,382 times\n",
      "  Up → No change: 1,340 times\n",
      "  Down → No change: 1,027 times\n",
      "  No change → Up: 985 times\n",
      "  Down → Down: 483 times\n",
      "  Up → Up: 442 times\n",
      "\n",
      "=== DIRECTION WITH VOLUME ANALYSIS ===\n",
      "Volume analysis by direction:\n",
      "               sum         mean  count\n",
      "direction                             \n",
      "Down       3889213  1178.906638   3299\n",
      "No change  2923497   658.297005   4441\n",
      "Up         4697461  1460.653296   3216\n",
      "\n",
      "=== DIRECTION WITH PRICE CHANGE MAGNITUDE ===\n",
      "Price change magnitude by direction:\n",
      "               mean       std   min   max\n",
      "direction                                \n",
      "Down      -0.091103  0.055096 -0.65 -0.05\n",
      "No change  0.000000  0.000000  0.00  0.00\n",
      "Up         0.093890  0.059423  0.05  0.75\n",
      "\n",
      "=== EXAMPLES OF EACH DIRECTION ===\n",
      "Sample 'Up' movements:\n",
      "               date  price  price_change  qty\n",
      "2025-08-07 09:15:01 386.75          0.45 1401\n",
      "2025-08-07 09:15:02 386.80          0.05 1795\n",
      "2025-08-07 09:15:02 386.95          0.15  741\n",
      "\n",
      "Sample 'Down' movements:\n",
      "               date  price  price_change  qty\n",
      "2025-08-07 09:15:01  386.3         -0.55  895\n",
      "2025-08-07 09:15:03  386.9         -0.05 2717\n",
      "2025-08-07 09:15:03  386.8         -0.10 9068\n",
      "\n",
      "Sample 'No change':\n",
      "               date  price  price_change   qty\n",
      "2025-08-07 09:15:00 386.85           NaN 65740\n",
      "2025-08-07 09:15:06 386.65           0.0  4519\n",
      "2025-08-07 09:15:09 386.00           0.0  1904\n",
      "\n",
      "=== DIRECTION SUMMARY ===\n",
      "✓ Direction column created successfully\n",
      "✓ Total rows: 10,956\n",
      "✓ Up movements: 3,216\n",
      "✓ Down movements: 3,299\n",
      "✓ No change: 4,441\n",
      "✓ Most common direction: No change\n",
      "✓ Ready for directional analysis and pattern recognition\n",
      "\n",
      "=== UPDATED DATAFRAME INFO ===\n",
      "Columns: ['date', 'price', 'qty', 'trnvr', 'cum_trnvr', 'date_only', 'time', 'hour', 'minute', 'second', 'price_change', 'direction']\n",
      "Shape: (10956, 12)\n"
     ]
    }
   ],
   "source": [
    "# Create direction column: \"Up\", \"Down\", \"No change\"\n",
    "print(\"=== DIRECTION COLUMN CREATION ===\")\n",
    "\n",
    "# Create direction column based on price_change\n",
    "df['direction'] = df['price_change'].apply(lambda x: \n",
    "    'Up' if x > 0 else \n",
    "    'Down' if x < 0 else \n",
    "    'No change'\n",
    ")\n",
    "\n",
    "# Display the first few rows to verify the direction column\n",
    "print(\"=== SAMPLE DATA WITH DIRECTION ===\")\n",
    "print(df[['date', 'price', 'price_change', 'direction', 'qty', 'trnvr']].head(10))\n",
    "\n",
    "# Count the occurrences of each direction\n",
    "print(f\"\\n=== DIRECTION DISTRIBUTION ===\")\n",
    "direction_counts = df['direction'].value_counts()\n",
    "print(direction_counts)\n",
    "\n",
    "# Calculate percentages\n",
    "total_rows = len(df)\n",
    "print(f\"\\n=== DIRECTION PERCENTAGES ===\")\n",
    "for direction, count in direction_counts.items():\n",
    "    percentage = (count / total_rows) * 100\n",
    "    print(f\"{direction}: {count:,} ({percentage:.2f}%)\")\n",
    "\n",
    "# Analyze direction patterns\n",
    "print(f\"\\n=== DIRECTION PATTERN ANALYSIS ===\")\n",
    "\n",
    "# Check for consecutive directions\n",
    "print(\"Direction sequence analysis:\")\n",
    "consecutive_up = 0\n",
    "consecutive_down = 0\n",
    "consecutive_no_change = 0\n",
    "max_consecutive_up = 0\n",
    "max_consecutive_down = 0\n",
    "max_consecutive_no_change = 0\n",
    "\n",
    "current_up = 0\n",
    "current_down = 0\n",
    "current_no_change = 0\n",
    "\n",
    "for direction in df['direction']:\n",
    "    if direction == 'Up':\n",
    "        current_up += 1\n",
    "        current_down = 0\n",
    "        current_no_change = 0\n",
    "        max_consecutive_up = max(max_consecutive_up, current_up)\n",
    "    elif direction == 'Down':\n",
    "        current_down += 1\n",
    "        current_up = 0\n",
    "        current_no_change = 0\n",
    "        max_consecutive_down = max(max_consecutive_down, current_down)\n",
    "    else:  # No change\n",
    "        current_no_change += 1\n",
    "        current_up = 0\n",
    "        current_down = 0\n",
    "        max_consecutive_no_change = max(max_consecutive_no_change, current_no_change)\n",
    "\n",
    "print(f\"Maximum consecutive 'Up' movements: {max_consecutive_up}\")\n",
    "print(f\"Maximum consecutive 'Down' movements: {max_consecutive_down}\")\n",
    "print(f\"Maximum consecutive 'No change': {max_consecutive_no_change}\")\n",
    "\n",
    "# Direction by time periods\n",
    "print(f\"\\n=== DIRECTION BY TIME PERIODS ===\")\n",
    "df['hour'] = df['date'].dt.hour\n",
    "hourly_direction = df.groupby('hour')['direction'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "print(\"Direction distribution by hour:\")\n",
    "print(hourly_direction)\n",
    "\n",
    "# Direction transitions (what follows what)\n",
    "print(f\"\\n=== DIRECTION TRANSITIONS ===\")\n",
    "transitions = []\n",
    "for i in range(1, len(df)):\n",
    "    prev_direction = df['direction'].iloc[i-1]\n",
    "    curr_direction = df['direction'].iloc[i]\n",
    "    transitions.append((prev_direction, curr_direction))\n",
    "\n",
    "transition_counts = pd.Series(transitions).value_counts().head(10)\n",
    "print(\"Most common direction transitions:\")\n",
    "for transition, count in transition_counts.items():\n",
    "    print(f\"  {transition[0]} → {transition[1]}: {count:,} times\")\n",
    "\n",
    "# Direction with volume analysis\n",
    "print(f\"\\n=== DIRECTION WITH VOLUME ANALYSIS ===\")\n",
    "direction_volume = df.groupby('direction')['qty'].agg(['sum', 'mean', 'count'])\n",
    "print(\"Volume analysis by direction:\")\n",
    "print(direction_volume)\n",
    "\n",
    "# Direction with price change magnitude\n",
    "print(f\"\\n=== DIRECTION WITH PRICE CHANGE MAGNITUDE ===\")\n",
    "direction_magnitude = df.groupby('direction')['price_change'].agg(['mean', 'std', 'min', 'max'])\n",
    "print(\"Price change magnitude by direction:\")\n",
    "print(direction_magnitude)\n",
    "\n",
    "# Show examples of each direction\n",
    "print(f\"\\n=== EXAMPLES OF EACH DIRECTION ===\")\n",
    "\n",
    "# Up movements\n",
    "up_examples = df[df['direction'] == 'Up'][['date', 'price', 'price_change', 'qty']].head(3)\n",
    "print(\"Sample 'Up' movements:\")\n",
    "print(up_examples.to_string(index=False))\n",
    "\n",
    "# Down movements\n",
    "down_examples = df[df['direction'] == 'Down'][['date', 'price', 'price_change', 'qty']].head(3)\n",
    "print(f\"\\nSample 'Down' movements:\")\n",
    "print(down_examples.to_string(index=False))\n",
    "\n",
    "# No change\n",
    "no_change_examples = df[df['direction'] == 'No change'][['date', 'price', 'price_change', 'qty']].head(3)\n",
    "print(f\"\\nSample 'No change':\")\n",
    "print(no_change_examples.to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n=== DIRECTION SUMMARY ===\")\n",
    "print(f\"✓ Direction column created successfully\")\n",
    "print(f\"✓ Total rows: {total_rows:,}\")\n",
    "print(f\"✓ Up movements: {direction_counts.get('Up', 0):,}\")\n",
    "print(f\"✓ Down movements: {direction_counts.get('Down', 0):,}\")\n",
    "print(f\"✓ No change: {direction_counts.get('No change', 0):,}\")\n",
    "print(f\"✓ Most common direction: {direction_counts.index[0]}\")\n",
    "print(f\"✓ Ready for directional analysis and pattern recognition\")\n",
    "\n",
    "# Display final dataframe info\n",
    "print(f\"\\n=== UPDATED DATAFRAME INFO ===\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6864362d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ROLLING AVERAGE CALCULATION ===\n",
      "Calculating rolling averages...\n",
      "=== SAMPLE DATA WITH ROLLING AVERAGES ===\n",
      "                  date   price  price_1min_avg  price_5min_avg  \\\n",
      "0  2025-08-07 09:15:00  386.85      386.850000      386.850000   \n",
      "1  2025-08-07 09:15:01  386.30      386.575000      386.575000   \n",
      "2  2025-08-07 09:15:01  386.75      386.633333      386.633333   \n",
      "3  2025-08-07 09:15:02  386.80      386.675000      386.675000   \n",
      "4  2025-08-07 09:15:02  386.95      386.730000      386.730000   \n",
      "5  2025-08-07 09:15:03  386.90      386.758333      386.758333   \n",
      "6  2025-08-07 09:15:03  386.80      386.764286      386.764286   \n",
      "7  2025-08-07 09:15:04  386.75      386.762500      386.762500   \n",
      "8  2025-08-07 09:15:04  386.90      386.777778      386.777778   \n",
      "9  2025-08-07 09:15:05  386.65      386.765000      386.765000   \n",
      "10 2025-08-07 09:15:05  386.75      386.763636      386.763636   \n",
      "11 2025-08-07 09:15:06  386.65      386.754167      386.754167   \n",
      "12 2025-08-07 09:15:06  386.55      386.738462      386.738462   \n",
      "13 2025-08-07 09:15:07  386.40      386.714286      386.714286   \n",
      "14 2025-08-07 09:15:07  386.10      386.673333      386.673333   \n",
      "\n",
      "    price_15min_avg    qty  \n",
      "0        386.850000  65740  \n",
      "1        386.575000    895  \n",
      "2        386.633333   1401  \n",
      "3        386.675000   1795  \n",
      "4        386.730000    741  \n",
      "5        386.758333   2717  \n",
      "6        386.764286   9068  \n",
      "7        386.762500   1141  \n",
      "8        386.777778   1798  \n",
      "9        386.765000   2092  \n",
      "10       386.763636   1679  \n",
      "11       386.754167   4519  \n",
      "12       386.738462   1316  \n",
      "13       386.714286   2858  \n",
      "14       386.673333    640  \n",
      "\n",
      "=== ROLLING AVERAGE STATISTICS ===\n",
      "\n",
      "price_1min_avg:\n",
      "  Min: ₹383.55\n",
      "  Max: ₹383.55\n",
      "  Mean: ₹386.55\n",
      "  Std: ₹1.56\n",
      "\n",
      "price_5min_avg:\n",
      "  Min: ₹383.68\n",
      "  Max: ₹383.68\n",
      "  Mean: ₹386.52\n",
      "  Std: ₹1.52\n",
      "\n",
      "price_15min_avg:\n",
      "  Min: ₹383.76\n",
      "  Max: ₹383.76\n",
      "  Mean: ₹386.49\n",
      "  Std: ₹1.45\n",
      "\n",
      "=== PRICE VS ROLLING AVERAGES ANALYSIS ===\n",
      "Price differences from rolling averages:\n",
      "  vs 1-min avg: Mean=0.01, Std=0.16\n",
      "  vs 5-min avg: Mean=0.04, Std=0.36\n",
      "  vs 15-min avg: Mean=0.08, Std=0.51\n",
      "\n",
      "=== MOVING AVERAGE CROSSOVER ANALYSIS ===\n",
      "Price above 1-min average: 5,353 times (48.9%)\n",
      "Price above 5-min average: 5,648 times (51.6%)\n",
      "Price above 15-min average: 5,863 times (53.5%)\n",
      "\n",
      "=== MOVING AVERAGE CROSSOVER SIGNALS ===\n",
      "1-min vs 5-min crossovers:\n",
      "  Bullish (1-min crosses above 5-min): 58\n",
      "  Bearish (1-min crosses below 5-min): 58\n",
      "\n",
      "5-min vs 15-min crossovers:\n",
      "  Bullish (5-min crosses above 15-min): 15\n",
      "  Bearish (5-min crosses below 15-min): 14\n",
      "\n",
      "=== SAMPLE CROSSOVER SIGNALS ===\n",
      "Recent 1-min vs 5-min crossovers:\n",
      "               date  price  price_1min_avg  price_5min_avg  ma_1min_5min_signal\n",
      "2025-08-07 09:15:00 386.85      386.850000      386.850000                  NaN\n",
      "2025-08-07 09:16:30 387.10      386.335149      386.333333                  1.0\n",
      "2025-08-07 09:24:03 389.00      389.454167      389.521384                 -1.0\n",
      "2025-08-07 09:31:32 389.05      388.816667      388.760656                  1.0\n",
      "2025-08-07 09:35:27 389.25      389.260000      389.283019                 -1.0\n",
      "2025-08-07 09:38:39 389.20      389.045833      389.041935                  1.0\n",
      "2025-08-07 09:42:04 389.15      389.200000      389.208824                 -1.0\n",
      "2025-08-07 09:46:33 389.00      388.833333      388.809848                  1.0\n",
      "2025-08-07 09:49:52 388.80      388.925000      388.947458                 -1.0\n",
      "2025-08-07 09:52:57 388.95      388.917647      388.917188                  1.0\n",
      "\n",
      "=== ROLLING AVERAGE TRENDS ===\n",
      "1-min moving average trends:\n",
      "  Upward trending periods: 5,275\n",
      "  Downward trending periods: 5,396\n",
      "\n",
      "=== ROLLING AVERAGE SUMMARY ===\n",
      "✓ 1-minute rolling average calculated\n",
      "✓ 5-minute rolling average calculated\n",
      "✓ 15-minute rolling average calculated\n",
      "✓ Crossover signals identified\n",
      "✓ Trend analysis completed\n",
      "✓ Ready for technical analysis and trading signals\n",
      "\n",
      "=== UPDATED DATAFRAME INFO ===\n",
      "Columns: ['date', 'price', 'qty', 'trnvr', 'cum_trnvr', 'date_only', 'time', 'hour', 'minute', 'second', 'price_change', 'direction', 'price_1min_avg', 'price_5min_avg', 'price_15min_avg', 'price_vs_1min', 'price_vs_5min', 'price_vs_15min', 'above_1min', 'above_5min', 'above_15min', 'ma_1min_5min_cross', 'ma_1min_5min_signal', 'ma_5min_15min_cross', 'ma_5min_15min_signal', 'ma_1min_slope', 'ma_5min_slope', 'ma_15min_slope']\n",
      "Shape: (10956, 28)\n",
      "New columns added:\n",
      "  ✓ price_1min_avg\n",
      "  ✓ price_5min_avg\n",
      "  ✓ price_15min_avg\n",
      "  ✓ price_vs_1min\n",
      "  ✓ price_vs_5min\n",
      "  ✓ price_vs_15min\n",
      "  ✓ above_1min\n",
      "  ✓ above_5min\n",
      "  ✓ above_15min\n",
      "  ✓ ma_1min_5min_cross\n",
      "  ✓ ma_1min_5min_signal\n",
      "  ✓ ma_5min_15min_cross\n",
      "  ✓ ma_5min_15min_signal\n",
      "  ✓ ma_1min_slope\n",
      "  ✓ ma_5min_slope\n",
      "  ✓ ma_15min_slope\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dindo\\AppData\\Local\\Temp\\ipykernel_484\\3402582214.py:14: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  df_temp['price_1min_avg'] = df_temp['price'].rolling(window='1T', min_periods=1).mean()\n",
      "C:\\Users\\dindo\\AppData\\Local\\Temp\\ipykernel_484\\3402582214.py:17: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  df_temp['price_5min_avg'] = df_temp['price'].rolling(window='5T', min_periods=1).mean()\n",
      "C:\\Users\\dindo\\AppData\\Local\\Temp\\ipykernel_484\\3402582214.py:20: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  df_temp['price_15min_avg'] = df_temp['price'].rolling(window='15T', min_periods=1).mean()\n"
     ]
    }
   ],
   "source": [
    "# Calculate rolling averages (1 min, 5 min, 15 min) for price\n",
    "print(\"=== ROLLING AVERAGE CALCULATION ===\")\n",
    "\n",
    "# First, ensure the dataframe is sorted by datetime\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Set datetime as index for time-based rolling operations\n",
    "df_temp = df.set_index('date')\n",
    "\n",
    "# Calculate rolling averages at different time intervals\n",
    "print(\"Calculating rolling averages...\")\n",
    "\n",
    "# 1-minute rolling average\n",
    "df_temp['price_1min_avg'] = df_temp['price'].rolling(window='1T', min_periods=1).mean()\n",
    "\n",
    "# 5-minute rolling average\n",
    "df_temp['price_5min_avg'] = df_temp['price'].rolling(window='5T', min_periods=1).mean()\n",
    "\n",
    "# 15-minute rolling average\n",
    "df_temp['price_15min_avg'] = df_temp['price'].rolling(window='15T', min_periods=1).mean()\n",
    "\n",
    "# Reset index to get back to normal dataframe format\n",
    "df = df_temp.reset_index()\n",
    "\n",
    "# Display sample data with rolling averages\n",
    "print(\"=== SAMPLE DATA WITH ROLLING AVERAGES ===\")\n",
    "print(df[['date', 'price', 'price_1min_avg', 'price_5min_avg', 'price_15min_avg', 'qty']].head(15))\n",
    "\n",
    "# Basic statistics of rolling averages\n",
    "print(f\"\\n=== ROLLING AVERAGE STATISTICS ===\")\n",
    "rolling_cols = ['price_1min_avg', 'price_5min_avg', 'price_15min_avg']\n",
    "\n",
    "for col in rolling_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Min: ₹{df[col].min():.2f}\")\n",
    "    print(f\"  Max: ₹{df[col].min():.2f}\")\n",
    "    print(f\"  Mean: ₹{df[col].mean():.2f}\")\n",
    "    print(f\"  Std: ₹{df[col].std():.2f}\")\n",
    "\n",
    "# Compare current price vs rolling averages\n",
    "print(f\"\\n=== PRICE VS ROLLING AVERAGES ANALYSIS ===\")\n",
    "\n",
    "# Calculate differences from rolling averages\n",
    "df['price_vs_1min'] = df['price'] - df['price_1min_avg']\n",
    "df['price_vs_5min'] = df['price'] - df['price_5min_avg']\n",
    "df['price_vs_15min'] = df['price'] - df['price_15min_avg']\n",
    "\n",
    "# Show statistics of these differences\n",
    "print(\"Price differences from rolling averages:\")\n",
    "print(f\"  vs 1-min avg: Mean={df['price_vs_1min'].mean():.2f}, Std={df['price_vs_1min'].std():.2f}\")\n",
    "print(f\"  vs 5-min avg: Mean={df['price_vs_5min'].mean():.2f}, Std={df['price_vs_5min'].std():.2f}\")\n",
    "print(f\"  vs 15-min avg: Mean={df['price_vs_15min'].mean():.2f}, Std={df['price_vs_15min'].std():.2f}\")\n",
    "\n",
    "# Identify when price is above/below each moving average\n",
    "print(f\"\\n=== MOVING AVERAGE CROSSOVER ANALYSIS ===\")\n",
    "\n",
    "# Price position relative to moving averages\n",
    "df['above_1min'] = df['price'] > df['price_1min_avg']\n",
    "df['above_5min'] = df['price'] > df['price_5min_avg']\n",
    "df['above_15min'] = df['price'] > df['price_15min_avg']\n",
    "\n",
    "# Count how many times price is above each moving average\n",
    "above_1min_count = df['above_1min'].sum()\n",
    "above_5min_count = df['above_5min'].sum()\n",
    "above_15min_count = df['above_15min'].sum()\n",
    "\n",
    "print(f\"Price above 1-min average: {above_1min_count:,} times ({(above_1min_count/len(df)*100):.1f}%)\")\n",
    "print(f\"Price above 5-min average: {above_5min_count:,} times ({(above_5min_count/len(df)*100):.1f}%)\")\n",
    "print(f\"Price above 15-min average: {above_15min_count:,} times ({(above_15min_count/len(df)*100):.1f}%)\")\n",
    "\n",
    "# Moving average crossover signals\n",
    "print(f\"\\n=== MOVING AVERAGE CROSSOVER SIGNALS ===\")\n",
    "\n",
    "# 1-min vs 5-min crossover\n",
    "df['ma_1min_5min_cross'] = (df['price_1min_avg'] > df['price_5min_avg']).astype(int)\n",
    "df['ma_1min_5min_signal'] = df['ma_1min_5min_cross'].diff()\n",
    "\n",
    "# 5-min vs 15-min crossover\n",
    "df['ma_5min_15min_cross'] = (df['price_5min_avg'] > df['price_15min_avg']).astype(int)\n",
    "df['ma_5min_15min_signal'] = df['ma_5min_15min_cross'].diff()\n",
    "\n",
    "# Count crossover signals\n",
    "bullish_1min_5min = (df['ma_1min_5min_signal'] == 1).sum()\n",
    "bearish_1min_5min = (df['ma_1min_5min_signal'] == -1).sum()\n",
    "\n",
    "bullish_5min_15min = (df['ma_5min_15min_signal'] == 1).sum()\n",
    "bearish_5min_15min = (df['ma_5min_15min_signal'] == -1).sum()\n",
    "\n",
    "print(f\"1-min vs 5-min crossovers:\")\n",
    "print(f\"  Bullish (1-min crosses above 5-min): {bullish_1min_5min}\")\n",
    "print(f\"  Bearish (1-min crosses below 5-min): {bearish_1min_5min}\")\n",
    "\n",
    "print(f\"\\n5-min vs 15-min crossovers:\")\n",
    "print(f\"  Bullish (5-min crosses above 15-min): {bullish_5min_15min}\")\n",
    "print(f\"  Bearish (5-min crosses below 15-min): {bearish_5min_15min}\")\n",
    "\n",
    "# Show sample of crossover signals\n",
    "print(f\"\\n=== SAMPLE CROSSOVER SIGNALS ===\")\n",
    "crossover_sample = df[df['ma_1min_5min_signal'] != 0][['date', 'price', 'price_1min_avg', 'price_5min_avg', 'ma_1min_5min_signal']].head(10)\n",
    "print(\"Recent 1-min vs 5-min crossovers:\")\n",
    "print(crossover_sample.to_string(index=False))\n",
    "\n",
    "# Rolling average trends\n",
    "print(f\"\\n=== ROLLING AVERAGE TRENDS ===\")\n",
    "\n",
    "# Calculate the slope of each moving average (trend direction)\n",
    "df['ma_1min_slope'] = df['price_1min_avg'].diff()\n",
    "df['ma_5min_slope'] = df['price_5min_avg'].diff()\n",
    "df['ma_15min_slope'] = df['price_15min_avg'].diff()\n",
    "\n",
    "# Count trending periods\n",
    "trending_1min_up = (df['ma_1min_slope'] > 0).sum()\n",
    "trending_1min_down = (df['ma_1min_slope'] < 0).sum()\n",
    "\n",
    "print(f\"1-min moving average trends:\")\n",
    "print(f\"  Upward trending periods: {trending_1min_up:,}\")\n",
    "print(f\"  Downward trending periods: {trending_1min_down:,}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n=== ROLLING AVERAGE SUMMARY ===\")\n",
    "print(f\"✓ 1-minute rolling average calculated\")\n",
    "print(f\"✓ 5-minute rolling average calculated\")\n",
    "print(f\"✓ 15-minute rolling average calculated\")\n",
    "print(f\"✓ Crossover signals identified\")\n",
    "print(f\"✓ Trend analysis completed\")\n",
    "print(f\"✓ Ready for technical analysis and trading signals\")\n",
    "\n",
    "# Display final dataframe info\n",
    "print(f\"\\n=== UPDATED DATAFRAME INFO ===\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"New columns added:\")\n",
    "new_cols = ['price_1min_avg', 'price_5min_avg', 'price_15min_avg', 'price_vs_1min', \n",
    "            'price_vs_5min', 'price_vs_15min', 'above_1min', 'above_5min', 'above_15min',\n",
    "            'ma_1min_5min_cross', 'ma_1min_5min_signal', 'ma_5min_15min_cross', \n",
    "            'ma_5min_15min_signal', 'ma_1min_slope', 'ma_5min_slope', 'ma_15min_slope']\n",
    "for col in new_cols:\n",
    "    if col in df.columns:\n",
    "        print(f\"  ✓ {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3afb2c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ROLLING VOLUME SUM CALCULATION ===\n",
      "Calculating rolling volume sums...\n",
      "=== SAMPLE DATA WITH ROLLING VOLUME SUMS ===\n",
      "                  date    qty  volume_1min_sum  volume_5min_sum  \\\n",
      "0  2025-08-07 09:15:00  65740          65740.0          65740.0   \n",
      "1  2025-08-07 09:15:01    895          66635.0          66635.0   \n",
      "2  2025-08-07 09:15:01   1401          68036.0          68036.0   \n",
      "3  2025-08-07 09:15:02   1795          69831.0          69831.0   \n",
      "4  2025-08-07 09:15:02    741          70572.0          70572.0   \n",
      "5  2025-08-07 09:15:03   2717          73289.0          73289.0   \n",
      "6  2025-08-07 09:15:03   9068          82357.0          82357.0   \n",
      "7  2025-08-07 09:15:04   1141          83498.0          83498.0   \n",
      "8  2025-08-07 09:15:04   1798          85296.0          85296.0   \n",
      "9  2025-08-07 09:15:05   2092          87388.0          87388.0   \n",
      "10 2025-08-07 09:15:05   1679          89067.0          89067.0   \n",
      "11 2025-08-07 09:15:06   4519          93586.0          93586.0   \n",
      "12 2025-08-07 09:15:06   1316          94902.0          94902.0   \n",
      "13 2025-08-07 09:15:07   2858          97760.0          97760.0   \n",
      "14 2025-08-07 09:15:07    640          98400.0          98400.0   \n",
      "\n",
      "    volume_15min_sum  volume_30min_sum  \n",
      "0            65740.0           65740.0  \n",
      "1            66635.0           66635.0  \n",
      "2            68036.0           68036.0  \n",
      "3            69831.0           69831.0  \n",
      "4            70572.0           70572.0  \n",
      "5            73289.0           73289.0  \n",
      "6            82357.0           82357.0  \n",
      "7            83498.0           83498.0  \n",
      "8            85296.0           85296.0  \n",
      "9            87388.0           87388.0  \n",
      "10           89067.0           89067.0  \n",
      "11           93586.0           93586.0  \n",
      "12           94902.0           94902.0  \n",
      "13           97760.0           97760.0  \n",
      "14           98400.0           98400.0  \n",
      "\n",
      "=== ROLLING VOLUME SUM STATISTICS ===\n",
      "\n",
      "volume_1min_sum:\n",
      "  Min: 3,271\n",
      "  Max: 240,694\n",
      "  Mean: 41,775\n",
      "  Std: 40,998\n",
      "\n",
      "volume_5min_sum:\n",
      "  Min: 44,608\n",
      "  Max: 751,790\n",
      "  Mean: 183,460\n",
      "  Std: 118,364\n",
      "\n",
      "volume_15min_sum:\n",
      "  Min: 65,740\n",
      "  Max: 1,421,831\n",
      "  Mean: 479,341\n",
      "  Std: 213,644\n",
      "\n",
      "volume_30min_sum:\n",
      "  Min: 65,740\n",
      "  Max: 1,828,633\n",
      "  Mean: 897,923\n",
      "  Std: 342,277\n",
      "\n",
      "=== VOLUME ANALYSIS BY TIME WINDOWS ===\n",
      "Volume comparison across timeframes:\n",
      "  1-min average: 41,775\n",
      "  5-min average: 183,460\n",
      "  15-min average: 479,341\n",
      "  30-min average: 897,923\n",
      "\n",
      "=== VOLUME INTENSITY ANALYSIS ===\n",
      "Volume per minute for each window:\n",
      "  1-min window: 41,775 per minute\n",
      "  5-min window: 36,692 per minute\n",
      "  15-min window: 31,956 per minute\n",
      "  30-min window: 29,931 per minute\n",
      "\n",
      "=== VOLUME SPIKE DETECTION ===\n",
      "volume_1min_sum: 629 periods above 123,771 (2σ threshold)\n",
      "volume_5min_sum: 374 periods above 420,188 (2σ threshold)\n",
      "volume_15min_sum: 334 periods above 906,628 (2σ threshold)\n",
      "volume_30min_sum: 726 periods above 1,582,477 (2σ threshold)\n",
      "\n",
      "=== EXAMPLES OF VOLUME SPIKES ===\n",
      "\n",
      "volume_1min_sum spikes (top 3):\n",
      "               date  volume_1min_sum  qty  price\n",
      "2025-08-07 09:15:09         123896.0 1904  386.0\n",
      "2025-08-07 09:15:10         124376.0  480  385.7\n",
      "2025-08-07 09:15:10         130704.0 6328  386.0\n",
      "\n",
      "volume_5min_sum spikes (top 3):\n",
      "               date  volume_5min_sum   qty  price\n",
      "2025-08-07 09:17:00         424009.0  6377 387.60\n",
      "2025-08-07 09:17:08         441708.0 17699 387.60\n",
      "2025-08-07 09:17:09         446868.0  5160 387.55\n",
      "\n",
      "volume_15min_sum spikes (top 3):\n",
      "               date  volume_15min_sum   qty  price\n",
      "2025-08-07 09:20:49          932902.0 43873 389.85\n",
      "2025-08-07 09:21:01          966465.0 33563 389.65\n",
      "2025-08-07 09:21:01          967610.0  1145 389.55\n",
      "\n",
      "volume_30min_sum spikes (top 3):\n",
      "               date  volume_30min_sum  qty  price\n",
      "2025-08-07 09:35:12         1583813.0 4419 389.25\n",
      "2025-08-07 09:35:19         1583994.0  181 389.20\n",
      "2025-08-07 09:35:27         1586242.0 2248 389.25\n",
      "\n",
      "=== VOLUME TREND ANALYSIS ===\n",
      "Volume 1Min:\n",
      "  Increasing: 7,218 periods\n",
      "  Decreasing: 3,706 periods\n",
      "Volume 5Min:\n",
      "  Increasing: 7,590 periods\n",
      "  Decreasing: 3,342 periods\n",
      "Volume 15Min:\n",
      "  Increasing: 7,651 periods\n",
      "  Decreasing: 3,280 periods\n",
      "Volume 30Min:\n",
      "  Increasing: 7,952 periods\n",
      "  Decreasing: 2,980 periods\n",
      "\n",
      "=== VOLUME-PRICE CORRELATION ===\n",
      "volume_1min_sum vs Price correlation: 0.1923\n",
      "volume_5min_sum vs Price correlation: 0.4227\n",
      "volume_15min_sum vs Price correlation: 0.5060\n",
      "volume_30min_sum vs Price correlation: 0.4976\n",
      "\n",
      "Volume vs Price Change correlation:\n",
      "volume_1min_sum vs Price Change correlation: 0.0107\n",
      "volume_5min_sum vs Price Change correlation: 0.0141\n",
      "volume_15min_sum vs Price Change correlation: 0.0065\n",
      "volume_30min_sum vs Price Change correlation: 0.0019\n",
      "\n",
      "=== TIME-BASED VOLUME ANALYSIS ===\n",
      "Total volume by hour:\n",
      "  09:00 - 09:59: 2,164,111\n",
      "  10:00 - 10:59: 1,436,774\n",
      "  11:00 - 11:59: 1,340,606\n",
      "  12:00 - 12:59: 1,462,840\n",
      "  13:00 - 13:59: 1,428,015\n",
      "  14:00 - 14:59: 1,985,212\n",
      "  15:00 - 15:59: 1,692,613\n",
      "\n",
      "=== ROLLING VS CUMULATIVE VOLUME ===\n",
      "Final cumulative volume: 11,466,759\n",
      "Final 30-min rolling volume: 1,692,613\n",
      "Rolling volume as % of total: 14.8%\n",
      "\n",
      "=== ROLLING VOLUME SUMMARY ===\n",
      "✓ 1-minute rolling volume sum calculated\n",
      "✓ 5-minute rolling volume sum calculated\n",
      "✓ 15-minute rolling volume sum calculated\n",
      "✓ 30-minute rolling volume sum calculated\n",
      "✓ Volume intensity analysis completed\n",
      "✓ Volume spike detection implemented\n",
      "✓ Volume-price correlation analyzed\n",
      "✓ Ready for volume-based analysis and trading signals\n",
      "\n",
      "=== UPDATED DATAFRAME INFO ===\n",
      "Columns: ['date', 'price', 'qty', 'trnvr', 'cum_trnvr', 'date_only', 'time', 'hour', 'minute', 'second', 'price_change', 'direction', 'price_1min_avg', 'price_5min_avg', 'price_15min_avg', 'price_vs_1min', 'price_vs_5min', 'price_vs_15min', 'above_1min', 'above_5min', 'above_15min', 'ma_1min_5min_cross', 'ma_1min_5min_signal', 'ma_5min_15min_cross', 'ma_5min_15min_signal', 'ma_1min_slope', 'ma_5min_slope', 'ma_15min_slope', 'volume_1min_sum', 'volume_5min_sum', 'volume_15min_sum', 'volume_30min_sum', 'volume_1min_per_min', 'volume_5min_per_min', 'volume_15min_per_min', 'volume_30min_per_min', 'volume_1min_trend', 'volume_5min_trend', 'volume_15min_trend', 'volume_30min_trend']\n",
      "Shape: (10956, 40)\n",
      "New volume columns added:\n",
      "  ✓ volume_1min_sum\n",
      "  ✓ volume_5min_sum\n",
      "  ✓ volume_15min_sum\n",
      "  ✓ volume_30min_sum\n",
      "  ✓ volume_1min_per_min\n",
      "  ✓ volume_5min_per_min\n",
      "  ✓ volume_15min_per_min\n",
      "  ✓ volume_30min_per_min\n",
      "  ✓ volume_1min_trend\n",
      "  ✓ volume_5min_trend\n",
      "  ✓ volume_15min_trend\n",
      "  ✓ volume_30min_trend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dindo\\AppData\\Local\\Temp\\ipykernel_484\\559120730.py:11: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  df_temp['volume_1min_sum'] = df_temp['qty'].rolling(window='1T', min_periods=1).sum()\n",
      "C:\\Users\\dindo\\AppData\\Local\\Temp\\ipykernel_484\\559120730.py:14: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  df_temp['volume_5min_sum'] = df_temp['qty'].rolling(window='5T', min_periods=1).sum()\n",
      "C:\\Users\\dindo\\AppData\\Local\\Temp\\ipykernel_484\\559120730.py:17: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  df_temp['volume_15min_sum'] = df_temp['qty'].rolling(window='15T', min_periods=1).sum()\n",
      "C:\\Users\\dindo\\AppData\\Local\\Temp\\ipykernel_484\\559120730.py:20: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  df_temp['volume_30min_sum'] = df_temp['qty'].rolling(window='30T', min_periods=1).sum()\n"
     ]
    }
   ],
   "source": [
    "# Calculate rolling sum of volume over time windows\n",
    "print(\"=== ROLLING VOLUME SUM CALCULATION ===\")\n",
    "\n",
    "# Ensure the dataframe is sorted by datetime and has datetime index\n",
    "df_temp = df.set_index('date')\n",
    "\n",
    "# Calculate rolling sum of volume at different time intervals\n",
    "print(\"Calculating rolling volume sums...\")\n",
    "\n",
    "# 1-minute rolling volume sum\n",
    "df_temp['volume_1min_sum'] = df_temp['qty'].rolling(window='1T', min_periods=1).sum()\n",
    "\n",
    "# 5-minute rolling volume sum\n",
    "df_temp['volume_5min_sum'] = df_temp['qty'].rolling(window='5T', min_periods=1).sum()\n",
    "\n",
    "# 15-minute rolling volume sum\n",
    "df_temp['volume_15min_sum'] = df_temp['qty'].rolling(window='15T', min_periods=1).sum()\n",
    "\n",
    "# 30-minute rolling volume sum\n",
    "df_temp['volume_30min_sum'] = df_temp['qty'].rolling(window='30T', min_periods=1).sum()\n",
    "\n",
    "# Reset index to get back to normal dataframe format\n",
    "df = df_temp.reset_index()\n",
    "\n",
    "# Display sample data with rolling volume sums\n",
    "print(\"=== SAMPLE DATA WITH ROLLING VOLUME SUMS ===\")\n",
    "print(df[['date', 'qty', 'volume_1min_sum', 'volume_5min_sum', 'volume_15min_sum', 'volume_30min_sum']].head(15))\n",
    "\n",
    "# Basic statistics of rolling volume sums\n",
    "print(f\"\\n=== ROLLING VOLUME SUM STATISTICS ===\")\n",
    "volume_sum_cols = ['volume_1min_sum', 'volume_5min_sum', 'volume_15min_sum', 'volume_30min_sum']\n",
    "\n",
    "for col in volume_sum_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Min: {df[col].min():,.0f}\")\n",
    "    print(f\"  Max: {df[col].max():,.0f}\")\n",
    "    print(f\"  Mean: {df[col].mean():,.0f}\")\n",
    "    print(f\"  Std: {df[col].std():,.0f}\")\n",
    "\n",
    "# Volume analysis by time windows\n",
    "print(f\"\\n=== VOLUME ANALYSIS BY TIME WINDOWS ===\")\n",
    "\n",
    "# Compare volume across different timeframes\n",
    "print(\"Volume comparison across timeframes:\")\n",
    "print(f\"  1-min average: {df['volume_1min_sum'].mean():,.0f}\")\n",
    "print(f\"  5-min average: {df['volume_5min_sum'].mean():,.0f}\")\n",
    "print(f\"  15-min average: {df['volume_15min_sum'].mean():,.0f}\")\n",
    "print(f\"  30-min average: {df['volume_30min_sum'].mean():,.0f}\")\n",
    "\n",
    "# Volume intensity analysis\n",
    "print(f\"\\n=== VOLUME INTENSITY ANALYSIS ===\")\n",
    "\n",
    "# Calculate volume per minute for each window\n",
    "df['volume_1min_per_min'] = df['volume_1min_sum'] / 1\n",
    "df['volume_5min_per_min'] = df['volume_5min_sum'] / 5\n",
    "df['volume_15min_per_min'] = df['volume_15min_sum'] / 15\n",
    "df['volume_30min_per_min'] = df['volume_30min_sum'] / 30\n",
    "\n",
    "print(\"Volume per minute for each window:\")\n",
    "print(f\"  1-min window: {df['volume_1min_per_min'].mean():,.0f} per minute\")\n",
    "print(f\"  5-min window: {df['volume_5min_per_min'].mean():,.0f} per minute\")\n",
    "print(f\"  15-min window: {df['volume_15min_per_min'].mean():,.0f} per minute\")\n",
    "print(f\"  30-min window: {df['volume_30min_per_min'].mean():,.0f} per minute\")\n",
    "\n",
    "# Volume spikes detection\n",
    "print(f\"\\n=== VOLUME SPIKE DETECTION ===\")\n",
    "\n",
    "# Find periods of unusually high volume (above 2 standard deviations)\n",
    "for col in volume_sum_cols:\n",
    "    mean_vol = df[col].mean()\n",
    "    std_vol = df[col].std()\n",
    "    threshold = mean_vol + 2 * std_vol\n",
    "    \n",
    "    high_volume_periods = (df[col] > threshold).sum()\n",
    "    print(f\"{col}: {high_volume_periods} periods above {threshold:,.0f} (2σ threshold)\")\n",
    "\n",
    "# Show examples of volume spikes\n",
    "print(f\"\\n=== EXAMPLES OF VOLUME SPIKES ===\")\n",
    "for col in volume_sum_cols:\n",
    "    mean_vol = df[col].mean()\n",
    "    std_vol = df[col].std()\n",
    "    threshold = mean_vol + 2 * std_vol\n",
    "    \n",
    "    spikes = df[df[col] > threshold][['date', col, 'qty', 'price']].head(3)\n",
    "    if len(spikes) > 0:\n",
    "        print(f\"\\n{col} spikes (top 3):\")\n",
    "        print(spikes.to_string(index=False))\n",
    "\n",
    "# Volume trend analysis\n",
    "print(f\"\\n=== VOLUME TREND ANALYSIS ===\")\n",
    "\n",
    "# Calculate volume trends (slopes) for each window\n",
    "df['volume_1min_trend'] = df['volume_1min_sum'].diff()\n",
    "df['volume_5min_trend'] = df['volume_5min_sum'].diff()\n",
    "df['volume_15min_trend'] = df['volume_15min_sum'].diff()\n",
    "df['volume_30min_trend'] = df['volume_30min_sum'].diff()\n",
    "\n",
    "# Count increasing vs decreasing volume periods\n",
    "for col in ['volume_1min_trend', 'volume_5min_trend', 'volume_15min_trend', 'volume_30min_trend']:\n",
    "    increasing = (df[col] > 0).sum()\n",
    "    decreasing = (df[col] < 0).sum()\n",
    "    window_name = col.replace('_trend', '').replace('_', ' ').title()\n",
    "    print(f\"{window_name}:\")\n",
    "    print(f\"  Increasing: {increasing:,} periods\")\n",
    "    print(f\"  Decreasing: {decreasing:,} periods\")\n",
    "\n",
    "# Volume vs Price correlation\n",
    "print(f\"\\n=== VOLUME-PRICE CORRELATION ===\")\n",
    "\n",
    "# Calculate correlation between volume and price for each timeframe\n",
    "for col in volume_sum_cols:\n",
    "    correlation = df[col].corr(df['price'])\n",
    "    print(f\"{col} vs Price correlation: {correlation:.4f}\")\n",
    "\n",
    "# Volume vs Price change correlation\n",
    "print(f\"\\nVolume vs Price Change correlation:\")\n",
    "for col in volume_sum_cols:\n",
    "    correlation = df[col].corr(df['price_change'])\n",
    "    print(f\"{col} vs Price Change correlation: {correlation:.4f}\")\n",
    "\n",
    "# Time-based volume analysis\n",
    "print(f\"\\n=== TIME-BASED VOLUME ANALYSIS ===\")\n",
    "\n",
    "# Volume by hour of the day\n",
    "df['hour'] = df['date'].dt.hour\n",
    "hourly_volume = df.groupby('hour')['qty'].sum()\n",
    "print(\"Total volume by hour:\")\n",
    "for hour, volume in hourly_volume.items():\n",
    "    print(f\"  {hour:02d}:00 - {hour:02d}:59: {volume:,}\")\n",
    "\n",
    "# Rolling volume vs cumulative volume comparison\n",
    "print(f\"\\n=== ROLLING VS CUMULATIVE VOLUME ===\")\n",
    "print(f\"Final cumulative volume: {df['cum_trnvr'].iloc[-1]/df['price'].iloc[-1]:,.0f}\")\n",
    "print(f\"Final 30-min rolling volume: {df['volume_30min_sum'].iloc[-1]:,.0f}\")\n",
    "print(f\"Rolling volume as % of total: {(df['volume_30min_sum'].iloc[-1]/(df['cum_trnvr'].iloc[-1]/df['price'].iloc[-1])*100):.1f}%\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n=== ROLLING VOLUME SUMMARY ===\")\n",
    "print(f\"✓ 1-minute rolling volume sum calculated\")\n",
    "print(f\"✓ 5-minute rolling volume sum calculated\")\n",
    "print(f\"✓ 15-minute rolling volume sum calculated\")\n",
    "print(f\"✓ 30-minute rolling volume sum calculated\")\n",
    "print(f\"✓ Volume intensity analysis completed\")\n",
    "print(f\"✓ Volume spike detection implemented\")\n",
    "print(f\"✓ Volume-price correlation analyzed\")\n",
    "print(f\"✓ Ready for volume-based analysis and trading signals\")\n",
    "\n",
    "# Display final dataframe info\n",
    "print(f\"\\n=== UPDATED DATAFRAME INFO ===\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"New volume columns added:\")\n",
    "volume_new_cols = ['volume_1min_sum', 'volume_5min_sum', 'volume_15min_sum', 'volume_30min_sum',\n",
    "                   'volume_1min_per_min', 'volume_5min_per_min', 'volume_15min_per_min', 'volume_30min_per_min',\n",
    "                   'volume_1min_trend', 'volume_5min_trend', 'volume_15min_trend', 'volume_30min_trend']\n",
    "for col in volume_new_cols:\n",
    "    if col in df.columns:\n",
    "        print(f\"  ✓ {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c267ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5adb35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465a5193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4fbac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82467e96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
